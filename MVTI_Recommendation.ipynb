{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhqrHwHOvW6a",
        "outputId": "a67330bd-036f-4e4b-b543-ea1a46d22b0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.9.0 in /usr/local/lib/python3.10/dist-packages (2.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.9.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.2)\n",
            "Collecting tensorflow-text==2.9.0\n",
            "  Using cached tensorflow_text-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.9.0) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.9.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.14.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text==2.9.0) (2.15.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.8.0->tensorflow-text==2.9.0)\n",
            "  Using cached tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "  Using cached tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.2.2)\n",
            "Installing collected packages: tf-keras, tensorflow-text\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed tensorflow-text-2.9.0 tf-keras-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.9.0\n",
        "!pip install tensorflow-text==2.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtSIdt0svqFk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pickle\n",
        "import joblib\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb7qeO94m6cb"
      },
      "source": [
        "# 1. 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhW8x28xwJ6k",
        "outputId": "0433c7a8-cfc8-4ee6-c5cd-13b6edd9b5f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18405 entries, 0 to 18404\n",
            "Data columns (total 14 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Title           18405 non-null  object \n",
            " 1   Runtime         18405 non-null  float64\n",
            " 2   Release Date    18405 non-null  object \n",
            " 3   Certification   15783 non-null  object \n",
            " 4   Genres          18405 non-null  object \n",
            " 5   Origin Country  18334 non-null  object \n",
            " 6   Overview        18405 non-null  object \n",
            " 7   Director        15274 non-null  object \n",
            " 8   Cast            17855 non-null  object \n",
            " 9   Providers       16331 non-null  object \n",
            " 10  Rating Value    18405 non-null  float64\n",
            " 11  Rating Count    18405 non-null  int64  \n",
            " 12  Poster URL      18405 non-null  object \n",
            " 13  Backdrop URLs   18405 non-null  object \n",
            "dtypes: float64(2), int64(1), object(11)\n",
            "memory usage: 2.0+ MB\n"
          ]
        }
      ],
      "source": [
        "# 왓챠피디아 크롤링 데이터\n",
        "data = pd.read_csv('/content/media_data.csv', encoding = 'utf-8')\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl0AT3NKwYN2",
        "outputId": "b9c941e6-afd5-4071-aedd-161b00b1049f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Title             Genres  \\\n",
            "0     성냥공장 소녀           코미디, 드라마   \n",
            "1     키다리 아저씨    코미디, 애니메이션, 드라마   \n",
            "2  주윤발의 행운의 별  코미디, 로맨스, 로맨틱 코미디   \n",
            "3        부귀병단            코미디, 액션   \n",
            "4        불가사리    SF, 액션, 공포, 코미디   \n",
            "\n",
            "                                            Overview  Rating Value  \\\n",
            "0  무능력하고 무표정한 얼굴의 엄마와 계부의 생활비를 위해 매일같이 성냥공장에서 기계처...           3.8   \n",
            "1  고아이지만 언제나 밝은 주디와 그런 주디를 도와주는 후견인 '키다리 아저씨'의 사랑...           4.2   \n",
            "2  재벌가 도련님 임보생은 재산 상속권을 조건으로 육촌 동생 진옥선과의 혼인을 강요당한...           3.2   \n",
            "3  1940년대 초, 일본의 침공으로 전 중국이 혼란에 빠져있을 무렵, 중국내 일본의 ...           2.9   \n",
            "4  네바다주 사막 한 가운데 있는 작은 마을에는 20명도 안되는 주민들이 서로 도우며 ...           3.3   \n",
            "\n",
            "   Rating Count  \n",
            "0          5278  \n",
            "1          1623  \n",
            "2           184  \n",
            "3           128  \n",
            "4         61592  \n",
            "Index(['Title', 'Genres', 'Overview', 'Rating Value', 'Rating Count'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# 필요한 컬럼(피처)만 선택해서 새로운 데이터셋 생성\n",
        "content = data[['Title', 'Genres', 'Overview', 'Rating Value', 'Rating Count']]\n",
        "\n",
        "print(content.head())\n",
        "print(content.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSJ-t3I3ynoP",
        "outputId": "0abe063e-101c-4fe1-ce12-e9a119e8ee1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18405 entries, 0 to 18404\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Title         18405 non-null  object \n",
            " 1   Genres        18405 non-null  object \n",
            " 2   Overview      18405 non-null  object \n",
            " 3   Rating Value  18405 non-null  float64\n",
            " 4   Rating Count  18405 non-null  int64  \n",
            "dtypes: float64(1), int64(1), object(3)\n",
            "memory usage: 719.1+ KB\n"
          ]
        }
      ],
      "source": [
        "content.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2lWKUsFn_qF"
      },
      "outputs": [],
      "source": [
        "# MBTI 500 데이터\n",
        "mbti = pd.read_csv('/content/MBTI 500.csv')\n",
        "print(mbti.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weT2OJcIo95_"
      },
      "source": [
        "# 2. LaBSE 모델 로드 및 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY4zanQTpCDf"
      },
      "source": [
        "- 사전 학습된 LaBSE 모델을 사용하여 텍스트 데이터를 고차원 벡터로 변환합니다.\n",
        "- 이 임베딩 모델은 다국어 문장 임베딩을 위한 모델로, 15개 언어만 사용하도록 하여 경량화된 모델입니다.\n",
        "- 고차원 벡터로 변환된 텍스트는 유사도 계산에 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b7h3viqyqJz",
        "outputId": "be9760e5-5f11-43d3-8845-d96f8840d49a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " sentences (InputLayer)         [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer_1 (KerasLayer)     {'input_type_ids':   0           ['sentences[0][0]']              \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128)}                                                          \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       {'pooled_output': (  219171840   ['keras_layer_1[0][0]',          \n",
            "                                None, 768),                       'keras_layer_1[0][1]',          \n",
            "                                 'encoder_outputs':               'keras_layer_1[0][2]']          \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'sequence_output':                                               \n",
            "                                 (None, 128, 768)}                                                \n",
            "                                                                                                  \n",
            " tf.math.l2_normalize (TFOpLamb  (None, 768)         0           ['keras_layer[0][12]']           \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 219,171,840\n",
            "Trainable params: 0\n",
            "Non-trainable params: 219,171,840\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# LaBSE 모델 로드\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# 텍스트를 고차원 벡터로 인코딩하는 모델 구성\n",
        "def build_embedding_model():\n",
        "    sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "    encoder_inputs = preprocessor(sentences)\n",
        "    sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "    normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "    return tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "embedding_model = build_embedding_model()\n",
        "embedding_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejzb7I9UytjT"
      },
      "outputs": [],
      "source": [
        "# 텍스트 임베딩 함수\n",
        "def embed_texts(texts, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        batch_tensors = tf.constant(batch_texts)\n",
        "        batch_embeddings = embedding_model(batch_tensors).numpy()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKClhJGXyzkB"
      },
      "outputs": [],
      "source": [
        "# MBTI 임베딩\n",
        "mbti_posts = mbti.groupby('type')['posts'].apply(lambda posts: ' '.join(posts)).tolist()\n",
        "mbti_embeddings = embed_texts(mbti_posts)\n",
        "mbti_index = mbti.groupby('type').groups.keys()\n",
        "mbti_embeddings_dict = dict(zip(mbti_index, mbti_embeddings))\n",
        "\n",
        "# 콘텐츠 줄거리 임베딩\n",
        "contents_texts = content['Overview'].tolist()\n",
        "contents_embeddings = embed_texts(contents_texts)\n",
        "contents_index = content['Title'].tolist()\n",
        "contents_embeddings_dict = dict(zip(contents_index, contents_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKMs2NAdy009"
      },
      "outputs": [],
      "source": [
        "# 콘텐츠 임베딩 파일 저장 및 로드\n",
        "with open('contents_embeddings_dict.pkl', 'wb') as f:\n",
        "    pickle.dump(contents_embeddings_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u8SL6utz1hs"
      },
      "outputs": [],
      "source": [
        "# MBTI 임베딩 파일 저장 및 로드\n",
        "with open('mbti_embeddings_dict.pkl', 'rb') as f:\n",
        "    mbti_embeddings_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBzWXgtd4qws"
      },
      "outputs": [],
      "source": [
        "# 콘텐츠 임베딩 로드\n",
        "with open('contents_embeddings_dict_0612.pkl', 'rb') as f:\n",
        "    contents_embeddings_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wpK_lFop7Uz"
      },
      "source": [
        "# 3. 콘텐츠 장르 원-핫 인코딩 / 데이터 표준화 / 콘텐츠 평점 개수 정규화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWlFEohSqGIX"
      },
      "source": [
        "- 콘텐츠 장르를 원-핫 인코딩하여 장르 정보를 벡터화합니다.\n",
        "- 원-핫 인코딩된 장르 벡터와 평점 데이터를 결합하여 학습 데이터를 생성합니다.\n",
        "- 데이터 표준화를 통해 모델 학습을 용이하게 합니다.\n",
        "- 대중성 점수로 사용하기 위해 콘텐츠 평점 개수를 정규화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1wlVo92zGkf"
      },
      "outputs": [],
      "source": [
        "# 콘텐츠 장르 원-핫 인코딩\n",
        "genres = content['Genres'].str.get_dummies(sep=', ')\n",
        "\n",
        "# 평점 데이터 결합\n",
        "cbf_model_input = np.hstack([genres.values, content['Rating Value'].values.reshape(-1, 1)])\n",
        "\n",
        "# 데이터 표준화\n",
        "cbf_scaler = StandardScaler()\n",
        "cbf_model_input_scaled = cbf_scaler.fit_transform(cbf_model_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YqyCGkXEHgc",
        "outputId": "8b1eb7ba-0796-489f-cf26-e69c094e21fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-e69302caf052>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  content['Normalized Popularity Score'] = scaler.fit_transform(content[['Rating Count']])\n"
          ]
        }
      ],
      "source": [
        "# 콘텐츠 평점 개수를 정규화하는 함수\n",
        "def normalize_popularity_score(content):\n",
        "    scaler = MinMaxScaler()\n",
        "    content['Normalized Popularity Score'] = scaler.fit_transform(content[['Rating Count']])\n",
        "    return content\n",
        "\n",
        "content = normalize_popularity_score(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tyFtBWPrRTk"
      },
      "source": [
        "# 4. 모델 학습 및 최적화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRVIyyTfrUQC"
      },
      "source": [
        "- 학습 데이터와 평가 데이터로 분리합니다.\n",
        "- GBM(Gradient Boosting Machine)을 사용하여 모델을 학습합니다.\n",
        "- Grid Search를 통해 최적의 하이퍼파라미터를 탐색하고, 최적의 모델을 선정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "larJZXFZzQ0U"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터와 평가 데이터 분리\n",
        "X_train, X_val, y_train, y_val = train_test_split(cbf_model_input_scaled, content['Rating Value'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 그리드 설정\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 300, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5, 6, 7],\n",
        "    'subsample': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# 그리드 서치 설정\n",
        "gbm = GradientBoostingRegressor(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=1, verbose=2)\n",
        "\n",
        "# 그리드 서치 수행\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 모델 출력\n",
        "best_gbm = grid_search.best_estimator_\n",
        "print(f'Best GBM Model: {best_gbm}')\n",
        "\n",
        "# 검증 데이터로 모델 성능 평가\n",
        "y_pred = best_gbm.predict(X_val)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f'Validation MSE: {mse}')\n",
        "\n",
        "# 모델 저장\n",
        "joblib.dump(best_gbm, 'best_gbm_model_final.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt8cQcHRzZ9y"
      },
      "outputs": [],
      "source": [
        "# 최적의 모델 로드\n",
        "best_gbm = joblib.load('best_gbm_model_final_0612.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUI9daX0roFL"
      },
      "source": [
        "# 5. 콘텐츠 추천 시스템 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4tBAQYerq9g"
      },
      "source": [
        "- **유사도 계산 함수**: 두 임베딩 벡터 간의 코사인 유사도를 계산합니다.\n",
        "- **MBTI 기반 추천**: 사용자의 MBTI 임베딩과 콘텐츠 임베딩 간의 유사도를 계산하여 콘텐츠를 추천합니다.\n",
        "- **선호 콘텐츠 기반 추천**: 사용자가 선호하는 콘텐츠와 유사한 콘텐츠를 추천합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAh5vYVSzcZr"
      },
      "outputs": [],
      "source": [
        "# 콘텐츠와 MBTI 유형 간의 유사도 계산 함수\n",
        "def calculate_similarity(content_embedding, user_embedding):\n",
        "    from numpy import dot\n",
        "    from numpy.linalg import norm\n",
        "    return dot(content_embedding, user_embedding) / (norm(content_embedding) * norm(user_embedding))\n",
        "\n",
        "# 1. MBTI 임베딩 기반 추천\n",
        "def recommend_contents_by_mbti(user_embedding, content_embeddings_dict, top_n=100):\n",
        "    recommended_contents = []\n",
        "    for content_id, embedding in content_embeddings_dict.items():\n",
        "        similarity = calculate_similarity(embedding, user_embedding)\n",
        "        recommended_contents.append((content_id, similarity))\n",
        "    recommended_contents = sorted(recommended_contents, key=lambda x: x[1], reverse=True)\n",
        "    return recommended_contents[:top_n]\n",
        "\n",
        "# 2. 선호 콘텐츠 유사도 기반 추천\n",
        "def recommend_similar_contents(preferred_contents, content_embeddings_dict, top_n=100):\n",
        "    similar_contents = {}\n",
        "    for content in preferred_contents:\n",
        "        content_embedding = content_embeddings_dict[content]\n",
        "        for other_content, embedding in content_embeddings_dict.items():\n",
        "            if other_content not in preferred_contents:\n",
        "                similarity = calculate_similarity(embedding, content_embedding)\n",
        "                if other_content not in similar_contents:\n",
        "                    similar_contents[other_content] = 0\n",
        "                similar_contents[other_content] += similarity\n",
        "    similar_contents = sorted(similar_contents.items(), key=lambda x: x[1], reverse=True)\n",
        "    return similar_contents[:top_n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5oiKzhus8vc"
      },
      "source": [
        "- **결합 추천 시스템**: MBTI 유사도 점수, 선호 콘텐츠 유사도 점수, 모델 점수, 대중성 점수에 각각 가중치를 부여하여 콘텐츠를 추천합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak2mPV0ZznU8"
      },
      "outputs": [],
      "source": [
        "# 결합 추천 시스템\n",
        "def recommend_contents_combined(user_embedding, preferred_contents, content_embeddings_dict, model, content, cbf_model_input_scaled, top_n=20):\n",
        "    weight_mbti = 0.3\n",
        "    weight_similar = 0.4\n",
        "    weight_model = 0.1\n",
        "    weight_popularity = 0.2\n",
        "\n",
        "    mbti_recommendations = recommend_contents_by_mbti(user_embedding, content_embeddings_dict, top_n=100)\n",
        "    similar_content_recommendations = recommend_similar_contents(preferred_contents, content_embeddings_dict, top_n=100)\n",
        "\n",
        "    combined_recommendations = {}\n",
        "\n",
        "    # MBTI 유사도 점수 보정\n",
        "    mbti_scores = [score for _, score in mbti_recommendations]\n",
        "    mbti_std = np.std(mbti_scores)\n",
        "    mbti_scores_normalized = [score / mbti_std for score in mbti_scores]\n",
        "\n",
        "    for (content_id, _), score in zip(mbti_recommendations, mbti_scores_normalized):\n",
        "        if content_id not in combined_recommendations:\n",
        "            combined_recommendations[content_id] = 0\n",
        "        combined_recommendations[content_id] += weight_mbti * score\n",
        "\n",
        "    # 선호 콘텐츠 유사도 점수 보정\n",
        "    similar_scores = [score for _, score in similar_content_recommendations]\n",
        "    similar_std = np.std(similar_scores)\n",
        "    similar_scores_normalized = [score / similar_std for score in similar_scores]\n",
        "\n",
        "    for (content_id, _), score in zip(similar_content_recommendations, similar_scores_normalized):\n",
        "        if content_id not in combined_recommendations:\n",
        "            combined_recommendations[content_id] = 0\n",
        "        combined_recommendations[content_id] += weight_similar * score\n",
        "\n",
        "    # 모델 점수 보정\n",
        "    content_indices = [content.index[content['Title'] == content_id][0] for content_id in combined_recommendations.keys()]\n",
        "    if content_indices:\n",
        "        content_features = cbf_model_input_scaled[content_indices]\n",
        "        model_scores = model.predict(content_features).flatten()\n",
        "        model_std = np.std(model_scores)\n",
        "        model_scores_normalized = [score / model_std for score in model_scores]\n",
        "\n",
        "        for content_id, model_score in zip(combined_recommendations.keys(), model_scores_normalized):\n",
        "            combined_recommendations[content_id] += weight_model * model_score\n",
        "\n",
        "    # 대중성 점수 보정\n",
        "    popularity_scores = [content.loc[content['Title'] == content_id, 'Normalized Popularity Score'].values[0] for content_id in combined_recommendations.keys()]\n",
        "    popularity_std = np.std(popularity_scores)\n",
        "    popularity_scores_normalized = [score / popularity_std for score in popularity_scores]\n",
        "\n",
        "    for content_id, popularity_score in zip(combined_recommendations.keys(), popularity_scores_normalized):\n",
        "        combined_recommendations[content_id] += weight_popularity * popularity_score\n",
        "\n",
        "    filtered_recommendations = {content_id: score for content_id, score in combined_recommendations.items()\n",
        "                                if content.loc[content['Title'] == content_id, 'Rating Value'].values[0] >= 3.8}\n",
        "\n",
        "    final_recommendations = sorted(filtered_recommendations.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [content_id for content_id, _ in final_recommendations[:top_n]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb0SzJyKuLcJ"
      },
      "source": [
        "- 재추천 시스템: 첫 번째 추천 목록에서 사용자가 마음에 드는(선택한) 콘텐츠를 기반으로 이전에 추천된 콘텐츠와 처음에 선호 콘텐츠로 선택한 콘텐츠를 제외하고 재추천을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5OZ2lPjzp3j"
      },
      "outputs": [],
      "source": [
        "# 재추천 시스템\n",
        "def recommend_contents_combined_excluding(user_embedding, preferred_contents, previous_recommendations, initial_preferred_contents, content_embeddings_dict, model, content, cbf_model_input_scaled, top_n=20):\n",
        "    # 가중치 설정\n",
        "    weight_mbti = 0.3\n",
        "    weight_similar = 0.4\n",
        "    weight_model = 0.1\n",
        "    weight_popularity = 0.2\n",
        "\n",
        "    # 초기 선호 콘텐츠와 현재 선호 콘텐츠 합치기\n",
        "    all_preferred_contents = list(set(preferred_contents + initial_preferred_contents))\n",
        "\n",
        "    # MBTI와 유사 콘텐츠 추천 상위 100개 가져오기\n",
        "    mbti_recommendations = recommend_contents_by_mbti(user_embedding, content_embeddings_dict, top_n=100)\n",
        "    similar_content_recommendations = recommend_similar_contents(all_preferred_contents, content_embeddings_dict, top_n=100)\n",
        "\n",
        "    # 추천 콘텐츠를 저장할 딕셔너리 초기화\n",
        "    combined_recommendations = {}\n",
        "\n",
        "    # 모든 추천 콘텐츠 ID 집합 생성 & 중복 제거\n",
        "    all_recommendations = set([content_id for content_id, _ in mbti_recommendations] +\n",
        "                              [content_id for content_id, _ in similar_content_recommendations])\n",
        "\n",
        "    # 이전에 추천된 콘텐츠와 사용자의 초기 선호 콘텐츠 필터링\n",
        "    filtered_content_ids = {content_id for content_id in all_recommendations\n",
        "                            if content_id not in previous_recommendations and\n",
        "                            content_id not in all_preferred_contents}\n",
        "\n",
        "    # 필터링된 콘텐츠의 추천 점수 초기화\n",
        "    for content_id in filtered_content_ids:\n",
        "        combined_recommendations[content_id] = 0\n",
        "\n",
        "    # MBTI 유사도 점수 보정\n",
        "    mbti_scores = [score for content_id, score in mbti_recommendations if content_id in filtered_content_ids]\n",
        "    mbti_std = np.std(mbti_scores) # std\n",
        "    mbti_scores_normalized = [score / mbti_std for score in mbti_scores]\n",
        "\n",
        "    # 필터링된 콘텐츠의 MBTI 점수 가중치 적용\n",
        "    for (content_id, _), score in zip(mbti_recommendations, mbti_scores_normalized):\n",
        "        if content_id in filtered_content_ids:\n",
        "            combined_recommendations[content_id] += weight_mbti * score\n",
        "\n",
        "    # 선호 콘텐츠 유사도 점수 보정\n",
        "    similar_scores = [score for content_id, score in similar_content_recommendations if content_id in filtered_content_ids]\n",
        "    similar_std = np.std(similar_scores)\n",
        "    similar_scores_normalized = [score / similar_std for score in similar_scores]\n",
        "\n",
        "    # 필터링된 콘텐츠의 유사 콘텐츠 점수 가중치 적용\n",
        "    for (content_id, _), score in zip(similar_content_recommendations, similar_scores_normalized):\n",
        "        if content_id in filtered_content_ids:\n",
        "            combined_recommendations[content_id] += weight_similar * score\n",
        "\n",
        "    # 모델 점수 보정\n",
        "    # 필터링된 콘텐츠 인덱스\n",
        "    content_indices = [content.index[content['Title'] == content_id][0] for content_id in filtered_content_ids]\n",
        "    if content_indices:\n",
        "        # 모델에 입력할 스케일링된 피쳐\n",
        "        content_features = cbf_model_input_scaled[content_indices]\n",
        "        model_scores = model.predict(content_features).flatten()\n",
        "        model_std = np.std(model_scores)\n",
        "        model_scores_normalized = [score / model_std for score in model_scores]\n",
        "\n",
        "        # 필터링된 콘텐츠의 모델 점수 가중치 적용\n",
        "        for content_id, model_score in zip(filtered_content_ids, model_scores_normalized):\n",
        "            combined_recommendations[content_id] += weight_model * model_score\n",
        "\n",
        "    # 대중성 점수 보정\n",
        "    # 필터링된 콘텐츠의 대중성 점수\n",
        "    popularity_scores = [content.loc[content['Title'] == content_id, 'Normalized Popularity Score'].values[0] for content_id in filtered_content_ids]\n",
        "    popularity_std = np.std(popularity_scores)\n",
        "    popularity_scores_normalized = [score / popularity_std for score in popularity_scores]\n",
        "\n",
        "    # 필터링된 콘텐츠의 대중성 점수 가중치 적용\n",
        "    for content_id, popularity_score in zip(filtered_content_ids, popularity_scores_normalized):\n",
        "        combined_recommendations[content_id] += weight_popularity * popularity_score\n",
        "\n",
        "    # 평점이 3.5 이상인 콘텐츠만 필터링\n",
        "    filtered_recommendations = {content_id: score for content_id, score in combined_recommendations.items()\n",
        "                                if content.loc[content['Title'] == content_id, 'Rating Value'].values[0] >= 3.5}\n",
        "\n",
        "    # 점수 내림차순으로 정렬 -> 최종 추천 콘텐츠 ID 리턴\n",
        "    final_recommendations = sorted(filtered_recommendations.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [content_id for content_id, _ in final_recommendations[:top_n]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "333f-hH6zrAs"
      },
      "outputs": [],
      "source": [
        "# 사용자 입력 추천 함수\n",
        "def user_input_recommendation(mbti_type, preferred_contents, content, cbf_model_input_scaled):\n",
        "    user_embedding = mbti_embeddings_dict[mbti_type]\n",
        "\n",
        "    # 최적화된 결합 추천 시스템 실행\n",
        "    recommendations = recommend_contents_combined(user_embedding, preferred_contents, contents_embeddings_dict, best_gbm, content, cbf_model_input_scaled)\n",
        "\n",
        "    return recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MospvC1BzthL",
        "outputId": "f53b4f7d-7ecf-4274-e63a-ef17c08a8b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "최적화된 결합 추천 콘텐츠 목록: ['매트릭스', '스파이더맨: 파 프롬 홈', '캡틴 아메리카: 윈터 솔져', '아이언맨 3', '스파이더맨 2', '스파이더맨', '나비 효과', '종말의 세라프', '프린세스 스타의 모험일기', '가디언즈 오브 갤럭시 Vol. 2', '시도니아의 기사 극장판', '노매드랜드', '중계지극해청뢰', '맨 인 블랙', '콘택트', '레미제라블: 뮤지컬 콘서트', '마법소녀를 동경해서', '바스터즈: 거친 녀석들', '부당거래', '나키의 저주: 용의 부활']\n",
            "최적화된 추천 시스템 실행 시간: 1.08 초\n"
          ]
        }
      ],
      "source": [
        "# 예시 사용자 입력 데이터\n",
        "new_user_mbti = \"INTP\"\n",
        "new_user_preferred_contents = ['어벤져스: 엔드게임', '어메이징 스파이더맨']\n",
        "\n",
        "# 최적화된 결합 추천 시스템 실행\n",
        "start_time = time.time()\n",
        "optimized_recommendations = user_input_recommendation(new_user_mbti, new_user_preferred_contents, content, cbf_model_input_scaled)\n",
        "end_time = time.time()\n",
        "print(\"최적화된 결합 추천 콘텐츠 목록:\", optimized_recommendations)\n",
        "print(f\"최적화된 추천 시스템 실행 시간: {end_time - start_time:.2f} 초\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3FWQGt6TS8k",
        "outputId": "444bb892-4c35-4545-8eb0-f28107da1957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "첫 번째 추천 목록에서 선호하는 콘텐츠를 쉼표로 구분하여 입력하세요: 매트릭스, 스파이더맨: 파 프롬 홈\n",
            "재추천 콘텐츠 목록: ['어벤져스', '스파이더맨 3', '오블리비언', '토르: 다크 월드', '어메이징 스파이더맨 2', '피니와 퍼브 무비: 2차원을 넘어서', '메이즈 러너', '매트릭스 3: 레볼루션', '맨 인 블랙 3', '어벤져스: 에이지 오브 울트론', '미스 페레그린과 이상한 아이들의 집', '터닝메카드', '더 기버: 기억전달자', '트랜스포머 프라임 비스트헌터: 프레데콘 라이징', '머나먼 세상속으로', '나소흑전기: 첫만남편', '장화신은 고양이: 끝내주는 모험', '드래곤볼 슈퍼', '낙원추방', '늑대소년']\n",
            "재추천 시스템 실행 시간: 1.31 초\n"
          ]
        }
      ],
      "source": [
        "# 첫 번째 추천으로부터 재추천\n",
        "liked_contents_input = input(\"첫 번째 추천 목록에서 선호하는 콘텐츠를 쉼표로 구분하여 입력하세요: \")\n",
        "liked_contents = [content.strip() for content in liked_contents_input.split(',')]\n",
        "\n",
        "previous_recommendations = optimized_recommendations\n",
        "new_preferred_contents = liked_contents  # 사용자 입력을 기반으로 새로운 선호 콘텐츠 목록 생성\n",
        "\n",
        "# 재추천 시스템 실행 시간 측정\n",
        "start_time = time.time()\n",
        "new_user_embedding = mbti_embeddings_dict[new_user_mbti]\n",
        "second_recommendations = recommend_contents_combined_excluding(new_user_embedding, new_preferred_contents, previous_recommendations, new_user_preferred_contents, contents_embeddings_dict, best_gbm, content, cbf_model_input_scaled)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"재추천 콘텐츠 목록:\", second_recommendations)\n",
        "print(f\"재추천 시스템 실행 시간: {end_time - start_time:.2f} 초\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Cfc2It-sgP"
      },
      "source": [
        "추천 개선을 위한 각 MBTI 유형의 TOP_20 추천 리스트 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nweVu_SiYw4X",
        "outputId": "4a1cb0f4-359b-48f5-930e-7617139478d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MBTI 유형: INTP\n",
            "1. 맨 인 블랙 (Score: 0.5141)\n",
            "2. 시리얼 킬러 연쇄살인범 (Score: 0.5112)\n",
            "3. 사경 (Score: 0.5039)\n",
            "4. 뮤지엄 (Score: 0.5012)\n",
            "5. 마법소녀를 동경해서 (Score: 0.5012)\n",
            "6. 외계+인 2부 (Score: 0.4962)\n",
            "7. 천재도둑 미스터A (Score: 0.4917)\n",
            "8. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.4888)\n",
            "9. 더 플레인 노랜딩 (Score: 0.4868)\n",
            "10. 빙하: 살인의 추억 (Score: 0.4851)\n",
            "11. 칠검 2 - 백발마녀전 (Score: 0.4818)\n",
            "12. VIP (Score: 0.4815)\n",
            "13. 비스트 스토커 (Score: 0.4810)\n",
            "14. 노 맨 오브 갓 (Score: 0.4800)\n",
            "15. 킬러 인 하이스쿨 (Score: 0.4800)\n",
            "16. 블랙 앤 화이트: 던 오브 저스티스 (Score: 0.4784)\n",
            "17. 나키의 저주: 용의 부활 (Score: 0.4777)\n",
            "18. 폭렬성시 (Score: 0.4761)\n",
            "19. 전쟁 (Score: 0.4758)\n",
            "20. 바스터즈: 거친 녀석들 (Score: 0.4747)\n",
            "\n",
            "\n",
            "MBTI 유형: ENTP\n",
            "1. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.5439)\n",
            "2. 시리얼 킬러 연쇄살인범 (Score: 0.5242)\n",
            "3. 어떻게든 되는 나날 (Score: 0.5171)\n",
            "4. 너를 줍다 (Score: 0.5107)\n",
            "5. 아적자위여해 : 나의 고슴도치 그녀 (Score: 0.5074)\n",
            "6. 오버 더 레인보우 (Score: 0.5067)\n",
            "7. 다운레인지 (Score: 0.5024)\n",
            "8. 고이 잠드소서 (Score: 0.4995)\n",
            "9. 당신이 잠든 사이 (Score: 0.4982)\n",
            "10. 령 (Score: 0.4928)\n",
            "11. 요출장안 (Score: 0.4895)\n",
            "12. 파 프롬 헤븐 (Score: 0.4888)\n",
            "13. 회사원 (Score: 0.4883)\n",
            "14. 사경 (Score: 0.4881)\n",
            "15. 서울연애 (Score: 0.4864)\n",
            "16. 나를 찾아줘 (Score: 0.4861)\n",
            "17. 백트랙 (Score: 0.4826)\n",
            "18. 팅커벨 3: 위대한 요정 구조대 (Score: 0.4811)\n",
            "19. 나를 잊지 말아요 (Score: 0.4810)\n",
            "20. 어느 멋진 날 (Score: 0.4809)\n",
            "\n",
            "\n",
            "MBTI 유형: INTJ\n",
            "1. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.4834)\n",
            "2. 평행이론: 도플갱어 살인 (Score: 0.4833)\n",
            "3. 맨 인 블랙 (Score: 0.4825)\n",
            "4. 마법의 제왕 (Score: 0.4751)\n",
            "5. 크리에이터 (Score: 0.4738)\n",
            "6. 매트릭스 (Score: 0.4672)\n",
            "7. 에일리언 2020 (Score: 0.4624)\n",
            "8. 나는 언제나 네가 지난 여름에 한 일을 알고 있을 것이다 (Score: 0.4606)\n",
            "9. VIP (Score: 0.4589)\n",
            "10. 크로니클 (Score: 0.4574)\n",
            "11. 발칙한 동거 빈방 있음 (Score: 0.4570)\n",
            "12. 외계+인 2부 (Score: 0.4570)\n",
            "13. 령 (Score: 0.4548)\n",
            "14. 요출장안 (Score: 0.4546)\n",
            "15. 악마들 (Score: 0.4534)\n",
            "16. 캠퍼스 코드 (Score: 0.4503)\n",
            "17. 배트맨 대 슈퍼맨: 저스티스의 시작 (Score: 0.4482)\n",
            "18. 레이싱 인 더 레인 (Score: 0.4474)\n",
            "19. 퍼펙트 스트레인저 (Score: 0.4469)\n",
            "20. 개목걸이 0 (Score: 0.4454)\n",
            "\n",
            "\n",
            "MBTI 유형: ENTJ\n",
            "1. 노 맨 오브 갓 (Score: 0.4890)\n",
            "2. 엑스 파일:  나는 믿고 싶다 (Score: 0.4818)\n",
            "3. 괴담동아리 (Score: 0.4781)\n",
            "4. 도날드 덕 가족의 모험 (Score: 0.4738)\n",
            "5. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.4691)\n",
            "6. 백트랙 (Score: 0.4626)\n",
            "7. 천 번을 불러도 (Score: 0.4611)\n",
            "8. 오큘러스 (Score: 0.4582)\n",
            "9. 플랫라이너 (Score: 0.4569)\n",
            "10. 바이오 테러리스트 (Score: 0.4561)\n",
            "11. 헐크 (Score: 0.4553)\n",
            "12. 아나토미 (Score: 0.4553)\n",
            "13. 크리미널 리벤지 (Score: 0.4548)\n",
            "14. 이쁜 것들이 되어라 (Score: 0.4545)\n",
            "15. 더크 젠틀리의 전체론적 탐정 사무소 (Score: 0.4522)\n",
            "16. 프론트 (Score: 0.4518)\n",
            "17. 머시블랙: 저주 받은 아이 (Score: 0.4507)\n",
            "18. 너를 줍다 (Score: 0.4504)\n",
            "19. 스틸 (Score: 0.4499)\n",
            "20. 떼시스: 예고 살인자 VS 최강 변호사 (Score: 0.4497)\n",
            "\n",
            "\n",
            "MBTI 유형: INFJ\n",
            "1. 서울연애 (Score: 0.5850)\n",
            "2. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.5563)\n",
            "3. 령 (Score: 0.5459)\n",
            "4. 어떻게든 되는 나날 (Score: 0.5391)\n",
            "5. 지난 여름, 갑자기 (Score: 0.5284)\n",
            "6. 하우 투 비 싱글 (Score: 0.5269)\n",
            "7. 어느 멋진 날 (Score: 0.5242)\n",
            "8. 파 프롬 헤븐 (Score: 0.5205)\n",
            "9. 메멘토 (Score: 0.5096)\n",
            "10. 통증 (Score: 0.5082)\n",
            "11. 낭만파 남편의 편지 (Score: 0.5059)\n",
            "12. 실버라이닝 플레이북 (Score: 0.5046)\n",
            "13. 마법사의 여명기 (Score: 0.5019)\n",
            "14. 당신이 잠든 사이 (Score: 0.4991)\n",
            "15. 두더지 (Score: 0.4976)\n",
            "16. VIP (Score: 0.4961)\n",
            "17. 브루스 올마이티 (Score: 0.4961)\n",
            "18. 시리얼 킬러 연쇄살인범 (Score: 0.4948)\n",
            "19. 비닐하우스 (Score: 0.4935)\n",
            "20. 크리스마스의 황당한 악몽 (Score: 0.4934)\n",
            "\n",
            "\n",
            "MBTI 유형: ENFJ\n",
            "1. 아적자위여해 : 나의 고슴도치 그녀 (Score: 0.5371)\n",
            "2. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.5317)\n",
            "3. 레이싱 인 더 레인 (Score: 0.5151)\n",
            "4. 끝에서 두번째 사랑 (Score: 0.5083)\n",
            "5. 령 (Score: 0.5005)\n",
            "6. 지난 여름, 갑자기 (Score: 0.4874)\n",
            "7. 어느 멋진 날 (Score: 0.4862)\n",
            "8. 러브, 사이먼 (Score: 0.4853)\n",
            "9. 여행자 (Score: 0.4840)\n",
            "10. 서울연애 (Score: 0.4833)\n",
            "11. 두더지 (Score: 0.4820)\n",
            "12. 파 프롬 헤븐 (Score: 0.4813)\n",
            "13. 평행이론: 도플갱어 살인 (Score: 0.4794)\n",
            "14. 당신이 잠든 사이 (Score: 0.4760)\n",
            "15. 백만장자의 첫사랑 (Score: 0.4758)\n",
            "16. 어떻게든 되는 나날 (Score: 0.4757)\n",
            "17. 사운드 오브 사일런스 (Score: 0.4749)\n",
            "18. 백트랙 (Score: 0.4747)\n",
            "19. 오버 더 레인보우 (Score: 0.4737)\n",
            "20. 너를 줍다 (Score: 0.4720)\n",
            "\n",
            "\n",
            "MBTI 유형: INFP\n",
            "1. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.5470)\n",
            "2. VIP (Score: 0.5306)\n",
            "3. 어떻게든 되는 나날 (Score: 0.5223)\n",
            "4. 통증 (Score: 0.5018)\n",
            "5. 서울연애 (Score: 0.4962)\n",
            "6. 레이싱 인 더 레인 (Score: 0.4953)\n",
            "7. 령 (Score: 0.4933)\n",
            "8. 어느 멋진 날 (Score: 0.4864)\n",
            "9. Re-BORN(리-본) (Score: 0.4831)\n",
            "10. 내 남자의 로맨스 (Score: 0.4776)\n",
            "11. 팬픽에서 연애까지 (Score: 0.4744)\n",
            "12. 구미호 가족 (Score: 0.4720)\n",
            "13. 아노말리사 (Score: 0.4718)\n",
            "14. 인형사 (Score: 0.4706)\n",
            "15. 나한테 XX해! (Score: 0.4697)\n",
            "16. 청구호전설 (Score: 0.4650)\n",
            "17. 이모티: 더 무비 (Score: 0.4636)\n",
            "18. 너를 줍다 (Score: 0.4635)\n",
            "19. 친구와 연인사이 (Score: 0.4634)\n",
            "20. 퍼퓸 (Score: 0.4633)\n",
            "\n",
            "\n",
            "MBTI 유형: ENFP\n",
            "1. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.5138)\n",
            "2. 령 (Score: 0.4958)\n",
            "3. 어떻게든 되는 나날 (Score: 0.4958)\n",
            "4. 팬픽에서 연애까지 (Score: 0.4849)\n",
            "5. 넌 어느 별에서 왔니 (Score: 0.4821)\n",
            "6. 천 번을 불러도 (Score: 0.4799)\n",
            "7. 싸이코패스 스토커 살인 (Score: 0.4746)\n",
            "8. 하우스 오브 스네일스 (Score: 0.4694)\n",
            "9. 리메모리 - 기억추출 (Score: 0.4671)\n",
            "10. 여중생A (Score: 0.4652)\n",
            "11. 브레인 게임 (Score: 0.4643)\n",
            "12. 아적자위여해 : 나의 고슴도치 그녀 (Score: 0.4611)\n",
            "13. 족구왕 (Score: 0.4608)\n",
            "14. 크로니클 (Score: 0.4592)\n",
            "15. 도날드 덕 가족의 모험 (Score: 0.4561)\n",
            "16. 나한테 XX해! (Score: 0.4557)\n",
            "17. 어서오시게스트하우스 (Score: 0.4549)\n",
            "18. 아이의 노랫소리를 들려줘 (Score: 0.4531)\n",
            "19. 늑대소년 (Score: 0.4525)\n",
            "20. 비밀의 언덕 (Score: 0.4505)\n",
            "\n",
            "\n",
            "MBTI 유형: ISTJ\n",
            "1. 악의 연대기 (Score: 0.4753)\n",
            "2. 끝에서 두번째 사랑 (Score: 0.4750)\n",
            "3. 파 프롬 헤븐 (Score: 0.4712)\n",
            "4. 령 (Score: 0.4704)\n",
            "5. 폭렬성시 (Score: 0.4682)\n",
            "6. 혼자 (Score: 0.4679)\n",
            "7. 시리얼 킬러 연쇄살인범 (Score: 0.4639)\n",
            "8. 레이싱 인 더 레인 (Score: 0.4629)\n",
            "9. 리메모리 - 기억추출 (Score: 0.4629)\n",
            "10. 모텔 라이프 (Score: 0.4596)\n",
            "11. 하우스 오브 스네일스 (Score: 0.4593)\n",
            "12. 메멘토 (Score: 0.4565)\n",
            "13. 사경 (Score: 0.4561)\n",
            "14. 비스트 스토커 (Score: 0.4560)\n",
            "15. 로맨틱 레시피 (Score: 0.4536)\n",
            "16. 귀곡성 : 귀신을 부르는 소리 (Score: 0.4521)\n",
            "17. 인시디어스: 두번째 집 (Score: 0.4516)\n",
            "18. 고이 잠드소서 (Score: 0.4515)\n",
            "19. 사후환생 (Score: 0.4514)\n",
            "20. 놉 (Score: 0.4503)\n",
            "\n",
            "\n",
            "MBTI 유형: ESTJ\n",
            "1. 붉은 달 푸른 해 (Score: 0.5175)\n",
            "2. 하늘이 기다려 (Score: 0.5022)\n",
            "3. 요출장안 (Score: 0.5020)\n",
            "4. 더 포리너 (Score: 0.5015)\n",
            "5. 불도저에 탄 소녀 (Score: 0.4905)\n",
            "6. 죄 많은 소녀 (Score: 0.4880)\n",
            "7. 주술회전 (Score: 0.4870)\n",
            "8. 나는 언제나 네가 지난 여름에 한 일을 알고 있을 것이다 (Score: 0.4861)\n",
            "9. 전쟁 (Score: 0.4859)\n",
            "10. 더 시크릿 (Score: 0.4846)\n",
            "11. 오렌지 카운티 (Score: 0.4818)\n",
            "12. 쓰리 데이즈 (Score: 0.4816)\n",
            "13. 모범생 (Score: 0.4795)\n",
            "14. 써스펙트 (Score: 0.4755)\n",
            "15. 오버 더 레인보우 (Score: 0.4743)\n",
            "16. 데몬 헌터 (Score: 0.4727)\n",
            "17. 어떻게든 되는 나날 (Score: 0.4723)\n",
            "18. 담쟁이 (Score: 0.4717)\n",
            "19. 레터스 투 줄리엣 (Score: 0.4716)\n",
            "20. 플루토에서 아침을 (Score: 0.4704)\n",
            "\n",
            "\n",
            "MBTI 유형: ISFJ\n",
            "1. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.5444)\n",
            "2. 지난 여름, 갑자기 (Score: 0.5410)\n",
            "3. 어떻게든 되는 나날 (Score: 0.5392)\n",
            "4. 서울연애 (Score: 0.5391)\n",
            "5. 팅커벨 3: 위대한 요정 구조대 (Score: 0.5202)\n",
            "6. 오렌지 카운티 (Score: 0.5190)\n",
            "7. 두더지 (Score: 0.5143)\n",
            "8. 령 (Score: 0.5109)\n",
            "9. 시저 밀란: 좋은 인간과 더 나은 개 (Score: 0.5069)\n",
            "10. 내겐 너무 특별한 친구 (Score: 0.5016)\n",
            "11. 평행이론: 도플갱어 살인 (Score: 0.5013)\n",
            "12. 썸머 타임머신 블루스 (Score: 0.5003)\n",
            "13. 러브, 사이먼 (Score: 0.4999)\n",
            "14. 고이 잠드소서 (Score: 0.4995)\n",
            "15. Re-BORN(리-본) (Score: 0.4979)\n",
            "16. 끝에서 두번째 사랑 (Score: 0.4933)\n",
            "17. 근거리 연애 (Score: 0.4921)\n",
            "18. VIP (Score: 0.4913)\n",
            "19. 천 번을 불러도 (Score: 0.4909)\n",
            "20. 내 남자의 로맨스 (Score: 0.4905)\n",
            "\n",
            "\n",
            "MBTI 유형: ESFJ\n",
            "1. 령 (Score: 0.5170)\n",
            "2. 파 프롬 헤븐 (Score: 0.5072)\n",
            "3. 천 번을 불러도 (Score: 0.4923)\n",
            "4. 평행이론: 도플갱어 살인 (Score: 0.4904)\n",
            "5. 너를 줍다 (Score: 0.4869)\n",
            "6. 더 타겟 (Score: 0.4819)\n",
            "7. 잠 (Score: 0.4794)\n",
            "8. 최후의 소녀 (Score: 0.4761)\n",
            "9. 아적자위여해 : 나의 고슴도치 그녀 (Score: 0.4750)\n",
            "10. 이상존재 (Score: 0.4747)\n",
            "11. 서울연애 (Score: 0.4733)\n",
            "12. 하우스 오브 스네일스 (Score: 0.4707)\n",
            "13. 내 남자의 로맨스 (Score: 0.4691)\n",
            "14. 어떻게든 되는 나날 (Score: 0.4655)\n",
            "15. 천국의 우편배달부 (Score: 0.4652)\n",
            "16. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.4648)\n",
            "17. VIP (Score: 0.4646)\n",
            "18. 건틀릿 (Score: 0.4631)\n",
            "19. 애프터데스 (Score: 0.4625)\n",
            "20. 빙하: 살인의 추억 (Score: 0.4622)\n",
            "\n",
            "\n",
            "MBTI 유형: ISTP\n",
            "1. 서울연애 (Score: 0.4848)\n",
            "2. 요출장안 (Score: 0.4569)\n",
            "3. 리빙: 어떤 인생 (Score: 0.4551)\n",
            "4. 어떻게든 되는 나날 (Score: 0.4524)\n",
            "5. 논스톱 플레인 (Score: 0.4474)\n",
            "6. 파 프롬 헤븐 (Score: 0.4449)\n",
            "7. 소스 코드 (Score: 0.4446)\n",
            "8. 천 번을 불러도 (Score: 0.4387)\n",
            "9. 생존: 러스트 크릭 (Score: 0.4375)\n",
            "10. 귀신을 보는 눈: 동안 (Score: 0.4365)\n",
            "11. 괜찮아요, 미스터 브래드 (Score: 0.4363)\n",
            "12. 백만장자의 첫사랑 (Score: 0.4361)\n",
            "13. 모텔 라이프 (Score: 0.4351)\n",
            "14. 아적자위여해 : 나의 고슴도치 그녀 (Score: 0.4346)\n",
            "15. 평행이론: 도플갱어 살인 (Score: 0.4336)\n",
            "16. 원령 (Score: 0.4325)\n",
            "17. 성자무쌍 ~샐러리맨이 이세계에서 살아남기 위해 걷는 길~ (Score: 0.4324)\n",
            "18. 구미호 가족 (Score: 0.4304)\n",
            "19. 화녀 (Score: 0.4295)\n",
            "20. 사이키 쿠스오의 재난 (Score: 0.4284)\n",
            "\n",
            "\n",
            "MBTI 유형: ESTP\n",
            "1. 끝에서 두번째 사랑 (Score: 0.4949)\n",
            "2. 트리니티 블러드 (Score: 0.4861)\n",
            "3. 디엠지:리로드 (Score: 0.4768)\n",
            "4. 서울연애 (Score: 0.4751)\n",
            "5. 상해전기 (Score: 0.4639)\n",
            "6. 구미호 가족 (Score: 0.4558)\n",
            "7. 하우스 오브 스네일스 (Score: 0.4537)\n",
            "8. 월드워Z (Score: 0.4525)\n",
            "9. 데스워치 (Score: 0.4522)\n",
            "10. 더 매치: 1944 (Score: 0.4493)\n",
            "11. 해커스 (Score: 0.4492)\n",
            "12. 9회말 2아웃 (Score: 0.4465)\n",
            "13. 더 프레지던트 (Score: 0.4455)\n",
            "14. 언유주얼 서스펙트 (Score: 0.4447)\n",
            "15. 백두산 (Score: 0.4444)\n",
            "16. 끝까지 사랑 (Score: 0.4437)\n",
            "17. 고이 잠드소서 (Score: 0.4434)\n",
            "18. 워 빌로우 (Score: 0.4411)\n",
            "19. 분노의 전차군단 (Score: 0.4367)\n",
            "20. 라스트 맨 다운 (Score: 0.4349)\n",
            "\n",
            "\n",
            "MBTI 유형: ISFP\n",
            "1. 노바디 (Score: 0.6033)\n",
            "2. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.5831)\n",
            "3. 필로포비아 (Score: 0.5636)\n",
            "4. 파 프롬 헤븐 (Score: 0.5635)\n",
            "5. 고이 잠드소서 (Score: 0.5625)\n",
            "6. 퍼펙트 스트레인저 (Score: 0.5598)\n",
            "7. 데빌 인사이드 : 악령의 집 (Score: 0.5562)\n",
            "8. 령 (Score: 0.5554)\n",
            "9. 지난 여름, 갑자기 (Score: 0.5553)\n",
            "10. 앤: 영혼이 깃든 인형 (Score: 0.5542)\n",
            "11. 어떻게든 되는 나날 (Score: 0.5532)\n",
            "12. 괴짜들의 로맨스 (Score: 0.5524)\n",
            "13. 나만 없어 고양이 (Score: 0.5519)\n",
            "14. 서울연애 (Score: 0.5512)\n",
            "15. 하드코어 디스코 (Score: 0.5508)\n",
            "16. 래터 (Score: 0.5471)\n",
            "17. 낭만파 남편의 편지 (Score: 0.5461)\n",
            "18. 플라이 미 투 더 문 (Score: 0.5456)\n",
            "19. 러브, 사이먼 (Score: 0.5444)\n",
            "20. 두더지 (Score: 0.5410)\n",
            "\n",
            "\n",
            "MBTI 유형: ESFP\n",
            "1. 어느 멋진 날 (Score: 0.5090)\n",
            "2. 어떻게든 되는 나날 (Score: 0.4793)\n",
            "3. 엄마와 나 그리고 나의 커밍아웃 (Score: 0.4731)\n",
            "4. 도날드 덕 가족의 모험 (Score: 0.4648)\n",
            "5. 돈키호테를 죽인 사나이 (Score: 0.4624)\n",
            "6. 천 번을 불러도 (Score: 0.4520)\n",
            "7. 이태원 살인사건 (Score: 0.4486)\n",
            "8. 미스터리 음악쇼 복면가왕 (Score: 0.4468)\n",
            "9. 파 프롬 헤븐 (Score: 0.4430)\n",
            "10. VIP (Score: 0.4412)\n",
            "11. 심야식당 (Score: 0.4377)\n",
            "12. 라이프 앤 베스 (Score: 0.4336)\n",
            "13. 배가본드 (Score: 0.4327)\n",
            "14. 사범 (Score: 0.4308)\n",
            "15. 시리얼 킬러 연쇄살인범 (Score: 0.4282)\n",
            "16. 요시찰 (Score: 0.4271)\n",
            "17. 성질 죽이기 (Score: 0.4266)\n",
            "18. 피아니스트 세이모어의 뉴욕 소네트 (Score: 0.4261)\n",
            "19. 스틸 (Score: 0.4259)\n",
            "20. 사랑이 이긴다 (Score: 0.4256)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 16가지 MBTI 유형\n",
        "mbti_types = [\"INTP\", \"ENTP\", \"INTJ\", \"ENTJ\", \"INFJ\", \"ENFJ\", \"INFP\", \"ENFP\",\n",
        "              \"ISTJ\", \"ESTJ\", \"ISFJ\", \"ESFJ\", \"ISTP\", \"ESTP\", \"ISFP\", \"ESFP\"]\n",
        "\n",
        "# 각 MBTI 유형에 대한 추천 리스트를 저장할 딕셔너리\n",
        "mbti_recommendations = {}\n",
        "\n",
        "# 각 MBTI 유형에 대해 추천 리스트 생성\n",
        "for mbti_type in mbti_types:\n",
        "    user_embedding = mbti_embeddings_dict[mbti_type]\n",
        "    recommendations = recommend_contents_by_mbti(user_embedding, contents_embeddings_dict, top_n=20)\n",
        "    mbti_recommendations[mbti_type] = recommendations\n",
        "\n",
        "# 각 MBTI 유형의 추천 리스트 출력\n",
        "for mbti_type, recommendations in mbti_recommendations.items():\n",
        "    print(f\"MBTI 유형: {mbti_type}\")\n",
        "    for rank, (title, score) in enumerate(recommendations, start=1):\n",
        "        print(f\"{rank}. {title} (Score: {score:.4f})\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8FE1Xle_Ezt"
      },
      "source": [
        "대중성 점수 필터링 처리 이후의 각 MBTI 유형의 TOP_20 추천 리스트 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ02HT0NiN4u",
        "outputId": "480dceb7-1980-4097-8591-1ab0c397b32a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MBTI 유형: INTP\n",
            "1. 맨 인 블랙\n",
            "2. 바스터즈: 거친 녀석들\n",
            "3. 아메리칸 사이코\n",
            "4. 부당거래\n",
            "5. 첫 키스만 50번째\n",
            "6. 걸캅스\n",
            "7. 감시자들\n",
            "8. 늑대소년\n",
            "9. 메멘토\n",
            "10. 수상한 그녀\n",
            "11. 그 남자의 기억법\n",
            "12. 몬스터 주식회사\n",
            "13. 나의 히어로 아카데미아\n",
            "14. 브이 포 벤데타\n",
            "15. 신세계\n",
            "16. 말아톤\n",
            "17. 원더\n",
            "18. 이미테이션 게임\n",
            "19. 달의 연인 - 보보경심 려\n",
            "20. 학교 2017\n",
            "\n",
            "\n",
            "MBTI 유형: ENTP\n",
            "1. 메멘토\n",
            "2. 헤어질 결심\n",
            "3. 파이트 클럽\n",
            "4. 늑대소녀와 흑왕자\n",
            "5. 시애틀의 잠 못 이루는 밤\n",
            "6. 그 남자의 기억법\n",
            "7. 연애의 온도\n",
            "8. 주술회전\n",
            "9. 하나와 앨리스\n",
            "10. 김종욱 찾기\n",
            "11. 내 아내의 모든 것\n",
            "12. 그 시절, 우리가 좋아했던 소녀\n",
            "13. 소스 코드\n",
            "14. 사랑의 블랙홀\n",
            "15. 데자뷰\n",
            "16. 옥탑방 고양이\n",
            "17. 시간을 달리는 소녀\n",
            "18. 원더풀 라이프\n",
            "19. 잭 리처\n",
            "20. 부당거래\n",
            "\n",
            "\n",
            "MBTI 유형: INTJ\n",
            "1. 맨 인 블랙\n",
            "2. 신과함께-죄와 벌\n",
            "3. 더 기버: 기억전달자\n",
            "4. 마이너리티 리포트\n",
            "5. 늑대소년\n",
            "6. 공동경비구역 JSA\n",
            "7. 몬스터 주식회사\n",
            "8. 주술회전\n",
            "9. 인셉션\n",
            "10. 본 얼티메이텀\n",
            "11. 바스터즈: 거친 녀석들\n",
            "12. 엣지 오브 투모로우\n",
            "13. 짱구는 못말려 극장판: 전설을 부르는 춤을 춰라, 아미고!\n",
            "14. 스파이더맨: 뉴 유니버스\n",
            "15. 청년경찰\n",
            "16. 검사외전\n",
            "17. 소스 코드\n",
            "18. 엑스맨\n",
            "19. 메멘토\n",
            "20. 거인\n",
            "\n",
            "\n",
            "MBTI 유형: ENTJ\n",
            "1. 아메리칸 사이코\n",
            "2. 당신, 거기 있어줄래요\n",
            "3. 애드 아스트라\n",
            "4. 아이, 로봇\n",
            "5. 아는 형님\n",
            "6. 극비수사\n",
            "7. 메멘토\n",
            "8. 인크레더블\n",
            "9. 콘택트\n",
            "10. 아멜리에\n",
            "11. 프리즈너스\n",
            "12. 어메이징 스파이더맨\n",
            "13. 검사외전\n",
            "14. 하나와 앨리스\n",
            "15. 천사와 악마\n",
            "16. 인셉션\n",
            "17. 스파이더맨\n",
            "18. 사카모토입니다만?\n",
            "19. 해리 포터와 죽음의 성물 2\n",
            "20. 21\n",
            "\n",
            "\n",
            "MBTI 유형: INFJ\n",
            "1. 메멘토\n",
            "2. 실버라이닝 플레이북\n",
            "3. 브루스 올마이티\n",
            "4. 옥탑방 고양이\n",
            "5. 파이트 클럽\n",
            "6. 탐정: 더 비기닝\n",
            "7. 나비 효과\n",
            "8. 뷰티 인사이드\n",
            "9. 캐쉬백\n",
            "10. 하나와 앨리스\n",
            "11. 500일의 썸머\n",
            "12. 원더풀 라이프\n",
            "13. 아메리칸 사이코\n",
            "14. 박물관이 살아있다!\n",
            "15. 도리를 찾아서\n",
            "16. 주술회전\n",
            "17. 바스터즈: 거친 녀석들\n",
            "18. 82년생 김지영\n",
            "19. 분노의 질주: 더 세븐\n",
            "20. 봉오동 전투\n",
            "\n",
            "\n",
            "MBTI 유형: ENFJ\n",
            "1. 시애틀의 잠 못 이루는 밤\n",
            "2. 죄 많은 소녀\n",
            "3. 연애의 온도\n",
            "4. 화이트 크리스마스\n",
            "5. 나비 효과\n",
            "6. 늑대소녀와 흑왕자\n",
            "7. 독전\n",
            "8. 런닝맨\n",
            "9. 그 시절, 우리가 좋아했던 소녀\n",
            "10. 스틸 앨리스\n",
            "11. 거인\n",
            "12. 메멘토\n",
            "13. 브루스 올마이티\n",
            "14. 용의자 X의 헌신\n",
            "15. 러브, 로지\n",
            "16. 심야식당\n",
            "17. 미씽: 사라진 여자\n",
            "18. 그 남자의 기억법\n",
            "19. 짱구는 못말려 극장판: 태풍을 부르는 장엄한 전설의 전투\n",
            "20. 콘택트\n",
            "\n",
            "\n",
            "MBTI 유형: INFP\n",
            "1. 김종욱 찾기\n",
            "2. 옥탑방 고양이\n",
            "3. 브루스 올마이티\n",
            "4. 에이 아이\n",
            "5. 아이, 로봇\n",
            "6. 네 멋대로 해라\n",
            "7. 이퀼리브리엄\n",
            "8. 주술회전\n",
            "9. 늑대소년\n",
            "10. 뷰티 인사이드\n",
            "11. 언터처블: 1%의 우정\n",
            "12. 더 기버: 기억전달자\n",
            "13. 아메리칸 사이코\n",
            "14. 인사이드 아웃\n",
            "15. 바스터즈: 거친 녀석들\n",
            "16. 말아톤\n",
            "17. 500일의 썸머\n",
            "18. 메멘토\n",
            "19. 나의 히어로 아카데미아\n",
            "20. 소울\n",
            "\n",
            "\n",
            "MBTI 유형: ENFP\n",
            "1. 늑대소년\n",
            "2. 메멘토\n",
            "3. 뷰티 인사이드\n",
            "4. 심야식당\n",
            "5. 에이 아이\n",
            "6. 리미트리스\n",
            "7. 짱구는 못말려 극장판: 태풍을 부르는 장엄한 전설의 전투\n",
            "8. 말아톤\n",
            "9. 탐정: 더 비기닝\n",
            "10. 애드 아스트라\n",
            "11. 나의 히어로 아카데미아\n",
            "12. 하나와 앨리스\n",
            "13. 김종욱 찾기\n",
            "14. 펜트하우스\n",
            "15. 화이트 크리스마스\n",
            "16. 그 시절, 우리가 좋아했던 소녀\n",
            "17. 옥탑방 고양이\n",
            "18. 탐정: 리턴즈\n",
            "19. 네 멋대로 해라\n",
            "20. 아메리칸 사이코\n",
            "\n",
            "\n",
            "MBTI 유형: ISTJ\n",
            "1. 메멘토\n",
            "2. 놉\n",
            "3. 신세계\n",
            "4. 형\n",
            "5. 부당거래\n",
            "6. 소스 코드\n",
            "7. 독전\n",
            "8. 고지전\n",
            "9. 주술회전\n",
            "10. 감시자들\n",
            "11. 인셉션\n",
            "12. 작은 아씨들\n",
            "13. 죄 많은 소녀\n",
            "14. 돈\n",
            "15. 탐정: 더 비기닝\n",
            "16. 브루스 올마이티\n",
            "17. 더 테러 라이브\n",
            "18. 탐정: 리턴즈\n",
            "19. 미션 임파서블: 폴아웃\n",
            "20. 아이, 로봇\n",
            "\n",
            "\n",
            "MBTI 유형: ESTJ\n",
            "1. 주술회전\n",
            "2. 죄 많은 소녀\n",
            "3. 레터스 투 줄리엣\n",
            "4. 시\n",
            "5. 지금, 만나러 갑니다\n",
            "6. 그 시절, 우리가 좋아했던 소녀\n",
            "7. 탐정: 더 비기닝\n",
            "8. 메멘토\n",
            "9. 검은 사제들\n",
            "10. 그 남자의 기억법\n",
            "11. 날씨의 아이\n",
            "12. 신과함께-죄와 벌\n",
            "13. 매기스 플랜\n",
            "14. 헬프\n",
            "15. 헤어질 결심\n",
            "16. 걸어도 걸어도\n",
            "17. 늑대소녀와 흑왕자\n",
            "18. 인디아나 존스: 크리스탈 해골의 왕국\n",
            "19. 화이트 크리스마스\n",
            "20. 언어의 정원\n",
            "\n",
            "\n",
            "MBTI 유형: ISFJ\n",
            "1. 원더풀 라이프\n",
            "2. 그 시절, 우리가 좋아했던 소녀\n",
            "3. 연애의 온도\n",
            "4. 지금, 만나러 갑니다\n",
            "5. 해리 포터와 비밀의 방\n",
            "6. 사랑의 블랙홀\n",
            "7. 원더\n",
            "8. 트루먼 쇼\n",
            "9. 우리들\n",
            "10. 헤어질 결심\n",
            "11. 나비 효과\n",
            "12. 팀 버튼의 크리스마스 악몽\n",
            "13. 그 남자의 기억법\n",
            "14. 키리시마가 동아리활동 그만둔대\n",
            "15. 시애틀의 잠 못 이루는 밤\n",
            "16. 심야식당\n",
            "17. 주술회전\n",
            "18. 학교 2017\n",
            "19. 설국열차\n",
            "20. 비포 선라이즈\n",
            "\n",
            "\n",
            "MBTI 유형: ESFJ\n",
            "1. 주술회전\n",
            "2. 네 멋대로 해라\n",
            "3. 레터스 투 줄리엣\n",
            "4. 13층\n",
            "5. 헤어질 결심\n",
            "6. 브루스 올마이티\n",
            "7. 콘택트\n",
            "8. 시애틀의 잠 못 이루는 밤\n",
            "9. 판의 미로: 오필리아와 세개의 열쇠\n",
            "10. 짱구는 못말려 극장판: 태풍을 부르는 장엄한 전설의 전투\n",
            "11. 월드워Z\n",
            "12. 사랑의 블랙홀\n",
            "13. 죄 많은 소녀\n",
            "14. 그 시절, 우리가 좋아했던 소녀\n",
            "15. 늑대소년\n",
            "16. 옥탑방 고양이\n",
            "17. 이지 A\n",
            "18. 기생수\n",
            "19. 친정엄마\n",
            "20. 유전\n",
            "\n",
            "\n",
            "MBTI 유형: ISTP\n",
            "1. 소스 코드\n",
            "2. 도리를 찾아서\n",
            "3. 센과 치히로의 행방불명\n",
            "4. 아멜리에\n",
            "5. 탐정: 더 비기닝\n",
            "6. 학교 2017\n",
            "7. 브루스 올마이티\n",
            "8. 주술회전\n",
            "9. 독전\n",
            "10. 살인의 추억\n",
            "11. 바스터즈: 거친 녀석들\n",
            "12. 파묘\n",
            "13. 러브 미 이프 유 데어\n",
            "14. 그 시절, 우리가 좋아했던 소녀\n",
            "15. 에너미 앳 더 게이트\n",
            "16. 소공녀\n",
            "17. 메멘토\n",
            "18. 설국열차\n",
            "19. 도그빌\n",
            "20. 신과함께-죄와 벌\n",
            "\n",
            "\n",
            "MBTI 유형: ESTP\n",
            "1. 월드워Z\n",
            "2. 바스터즈: 거친 녀석들\n",
            "3. 커피프린스 1호점\n",
            "4. 신세계\n",
            "5. 비포 선라이즈\n",
            "6. 혹성탈출: 반격의 서막\n",
            "7. 블레이드\n",
            "8. 혹성탈출: 종의 전쟁\n",
            "9. 나의 히어로 아카데미아\n",
            "10. 이미테이션 게임\n",
            "11. 죄 많은 소녀\n",
            "12. 옥탑방 고양이\n",
            "13. 에이 아이\n",
            "14. 한자와 나오키\n",
            "15. 본 아이덴티티\n",
            "16. 도리를 찾아서\n",
            "17. 줄무늬 파자마를 입은 소년\n",
            "18. 주술회전\n",
            "19. 미녀와 야수\n",
            "20. 헤어질 결심\n",
            "\n",
            "\n",
            "MBTI 유형: ISFP\n",
            "1. 우리들\n",
            "2. 네 멋대로 해라\n",
            "3. 도망자\n",
            "4. 극비수사\n",
            "5. 내가 널 사랑할 수 없는 10가지 이유\n",
            "6. 원더풀 라이프\n",
            "7. 개를 훔치는 완벽한 방법\n",
            "8. 내 아내의 모든 것\n",
            "9. 검은 사제들\n",
            "10. 늑대소녀와 흑왕자\n",
            "11. 연애의 온도\n",
            "12. 바람\n",
            "13. 도리를 찾아서\n",
            "14. 늑대소년\n",
            "15. 이별계약\n",
            "16. 지금, 만나러 갑니다\n",
            "17. 몬스터 주식회사\n",
            "18. 완벽한 타인\n",
            "19. 나비 효과\n",
            "20. 미씽: 사라진 여자\n",
            "\n",
            "\n",
            "MBTI 유형: ESFP\n",
            "1. 심야식당\n",
            "2. 주술회전\n",
            "3. 아메리칸 사이코\n",
            "4. 제빵왕 김탁구\n",
            "5. 옥탑방 고양이\n",
            "6. 그 남자의 기억법\n",
            "7. 뷰티 인사이드\n",
            "8. 극비수사\n",
            "9. 늑대소녀와 흑왕자\n",
            "10. 위대한 쇼맨\n",
            "11. 스타게이트\n",
            "12. 리멤버 - 아들의 전쟁\n",
            "13. 신비한 TV 서프라이즈\n",
            "14. 유주얼 서스펙트\n",
            "15. 말할 수 없는 비밀\n",
            "16. 네고시에이터\n",
            "17. 내가 널 사랑할 수 없는 10가지 이유\n",
            "18. 소스 코드\n",
            "19. 죄 많은 소녀\n",
            "20. 5시부터 9시까지 ~나를 사랑한 스님~\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def recommend_contents_combined_with_popularity_filter(user_embedding, preferred_contents, content_embeddings_dict, model, content, cbf_model_input_scaled, top_n=20):\n",
        "    weight_mbti = 0.4\n",
        "    weight_similar = 0.5\n",
        "    weight_model = 0.1\n",
        "\n",
        "    # 대중성 점수 필터링 (Normalized Popularity Score 기준 상위 90%)\n",
        "    popularity_threshold = content['Normalized Popularity Score'].quantile(0.9)\n",
        "    popular_content = content[content['Normalized Popularity Score'] >= popularity_threshold]\n",
        "    popular_content_embeddings_dict = {title: emb for title, emb in content_embeddings_dict.items() if title in popular_content['Title'].values}\n",
        "\n",
        "    mbti_recommendations = recommend_contents_by_mbti(user_embedding, popular_content_embeddings_dict, top_n=100)\n",
        "    similar_content_recommendations = recommend_similar_contents(preferred_contents, popular_content_embeddings_dict, top_n=100)\n",
        "\n",
        "    combined_recommendations = {}\n",
        "\n",
        "    # MBTI 유사도 점수 보정\n",
        "    mbti_scores = [score for _, score in mbti_recommendations]\n",
        "    if len(mbti_scores) > 1:\n",
        "        mbti_std = np.std(mbti_scores)\n",
        "        mbti_scores_normalized = [score / mbti_std for score in mbti_scores] if mbti_std != 0 else mbti_scores\n",
        "    else:\n",
        "        mbti_scores_normalized = mbti_scores\n",
        "\n",
        "    for (content_id, _), score in zip(mbti_recommendations, mbti_scores_normalized):\n",
        "        if content_id not in combined_recommendations:\n",
        "            combined_recommendations[content_id] = 0\n",
        "        combined_recommendations[content_id] += weight_mbti * score\n",
        "\n",
        "    # 선호 콘텐츠 유사도 점수 보정\n",
        "    similar_scores = [score for _, score in similar_content_recommendations]\n",
        "    if len(similar_scores) > 1:\n",
        "        similar_std = np.std(similar_scores)\n",
        "        similar_scores_normalized = [score / similar_std for score in similar_scores] if similar_std != 0 else similar_scores\n",
        "    else:\n",
        "        similar_scores_normalized = similar_scores\n",
        "\n",
        "    for (content_id, _), score in zip(similar_content_recommendations, similar_scores_normalized):\n",
        "        if content_id not in combined_recommendations:\n",
        "            combined_recommendations[content_id] = 0\n",
        "        combined_recommendations[content_id] += weight_similar * score\n",
        "\n",
        "    # 모델 점수 보정\n",
        "    content_indices = [content.index[content['Title'] == content_id][0] for content_id in combined_recommendations.keys() if content_id in content['Title'].values]\n",
        "    if content_indices:\n",
        "        content_features = cbf_model_input_scaled[content_indices]\n",
        "        model_scores = model.predict(content_features).flatten()\n",
        "        if len(model_scores) > 1:\n",
        "            model_std = np.std(model_scores)\n",
        "            model_scores_normalized = [score / model_std for score in model_scores] if model_std != 0 else model_scores\n",
        "        else:\n",
        "            model_scores_normalized = model_scores\n",
        "\n",
        "        for content_id, model_score in zip(combined_recommendations.keys(), model_scores_normalized):\n",
        "            combined_recommendations[content_id] += weight_model * model_score\n",
        "\n",
        "    filtered_recommendations = {content_id: score for content_id, score in combined_recommendations.items()\n",
        "                                if content.loc[content['Title'] == content_id, 'Rating Value'].values[0] >= 3.5}\n",
        "\n",
        "    final_recommendations = sorted(filtered_recommendations.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [content_id for content_id, _ in final_recommendations[:top_n]]\n",
        "\n",
        "# 16가지 MBTI 유형\n",
        "mbti_types = [\"INTP\", \"ENTP\", \"INTJ\", \"ENTJ\", \"INFJ\", \"ENFJ\", \"INFP\", \"ENFP\",\n",
        "              \"ISTJ\", \"ESTJ\", \"ISFJ\", \"ESFJ\", \"ISTP\", \"ESTP\", \"ISFP\", \"ESFP\"]\n",
        "\n",
        "# 각 MBTI 유형에 대한 추천 리스트를 저장할 딕셔너리\n",
        "mbti_recommendations = {}\n",
        "\n",
        "# 각 MBTI 유형에 대해 추천 리스트 생성\n",
        "for mbti_type in mbti_types:\n",
        "    user_embedding = mbti_embeddings_dict[mbti_type]\n",
        "    recommendations = recommend_contents_combined_with_popularity_filter(user_embedding, [], contents_embeddings_dict, best_gbm, content, cbf_model_input_scaled, top_n=20)\n",
        "    mbti_recommendations[mbti_type] = recommendations\n",
        "\n",
        "# 각 MBTI 유형의 추천 리스트 출력\n",
        "for mbti_type, recommendations in mbti_recommendations.items():\n",
        "    print(f\"MBTI 유형: {mbti_type}\")\n",
        "    for rank, title in enumerate(recommendations, start=1):\n",
        "        print(f\"{rank}. {title}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJAHoDjI_VLs"
      },
      "source": [
        "- 대중성 점수 가중치를 두는 것 보다 대중성 점수로 필터링을 한 이후에 각각의 유사도 점수를 구하는 것이 더 정확하고 적합한 추천을 가능하게 할 것이라고 사료됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXc28nBi_kF4"
      },
      "source": [
        "**최종 추천 시스템**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlG_dByF2AP-",
        "outputId": "a5066057-bc7f-41e2-f189-ffbb988b96d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "대중성 필터링을 추가한 추천 콘텐츠 목록: ['달의 연인 - 보보경심 려', '본 얼티메이텀', '소스 코드', '해리 포터와 죽음의 성물 2', '반지의 제왕: 반지 원정대', '맨 인 블랙', '콘스탄틴', '헝거게임: 캣칭 파이어', '서유기: 선리기연', '드래곤 길들이기 3', '블랙 위도우', '반지의 제왕: 두 개의 탑', '어바웃 타임', '반지의 제왕: 왕의 귀환', '말레피센트', '브루스 올마이티', '스파이더맨 2', '겨울왕국', '원펀맨', '하이힐']\n",
            "추천 시스템 실행 시간: 1.36 초\n"
          ]
        }
      ],
      "source": [
        "# 최종 추천 시스템\n",
        "def recommend_contents_combined_with_popularity_filter(user_embedding, preferred_contents, content_embeddings_dict, model, content, cbf_model_input_scaled, top_n=20):\n",
        "    weight_mbti = 0.4\n",
        "    weight_similar = 0.5\n",
        "    weight_model = 0.1\n",
        "\n",
        "    # 대중성 점수 필터링 (Normalized Popularity Score 기준 상위 90%)\n",
        "    popularity_threshold = content['Normalized Popularity Score'].quantile(0.9)\n",
        "    popular_content = content[content['Normalized Popularity Score'] >= popularity_threshold]\n",
        "\n",
        "    # 사용자가 선호하는 콘텐츠는 필터링에서 제외하고 포함시킴\n",
        "    popular_content_embeddings_dict = {title: emb for title, emb in content_embeddings_dict.items() if title in popular_content['Title'].values or title in preferred_contents}\n",
        "\n",
        "    mbti_recommendations = recommend_contents_by_mbti(user_embedding, popular_content_embeddings_dict, top_n=100)\n",
        "\n",
        "    # 유사도 계산\n",
        "    similar_contents = {}\n",
        "    for preferred_content in preferred_contents:\n",
        "        if preferred_content in content_embeddings_dict:\n",
        "            preferred_embedding = content_embeddings_dict[preferred_content]\n",
        "            for other_content, other_embedding in popular_content_embeddings_dict.items():\n",
        "                if other_content not in preferred_contents:\n",
        "                    similarity = np.dot(preferred_embedding, other_embedding) / (np.linalg.norm(preferred_embedding) * np.linalg.norm(other_embedding))\n",
        "                    if other_content not in similar_contents:\n",
        "                        similar_contents[other_content] = 0\n",
        "                    similar_contents[other_content] += similarity\n",
        "\n",
        "    similar_content_recommendations = sorted(similar_contents.items(), key=lambda x: x[1], reverse=True)[:100]\n",
        "\n",
        "    combined_recommendations = {}\n",
        "\n",
        "    # MBTI 유사도 점수 보정\n",
        "    mbti_scores = [score for _, score in mbti_recommendations]\n",
        "    mbti_std = np.std(mbti_scores)\n",
        "    mbti_scores_normalized = [score / mbti_std for score in mbti_scores]\n",
        "\n",
        "    for (content_id, _), score in zip(mbti_recommendations, mbti_scores_normalized):\n",
        "        if content_id not in combined_recommendations:\n",
        "            combined_recommendations[content_id] = 0\n",
        "        combined_recommendations[content_id] += weight_mbti * score\n",
        "\n",
        "    # 선호 콘텐츠 유사도 점수 보정\n",
        "    similar_scores = [score for _, score in similar_content_recommendations]\n",
        "    similar_std = np.std(similar_scores)\n",
        "    similar_scores_normalized = [score / similar_std for score in similar_scores]\n",
        "\n",
        "    for (content_id, _), score in zip(similar_content_recommendations, similar_scores_normalized):\n",
        "        if content_id not in combined_recommendations:\n",
        "            combined_recommendations[content_id] = 0\n",
        "        combined_recommendations[content_id] += weight_similar * score\n",
        "\n",
        "    # 모델 점수 보정\n",
        "    content_indices = [content.index[content['Title'] == content_id][0] for content_id in combined_recommendations.keys()]\n",
        "    if content_indices:\n",
        "        content_features = cbf_model_input_scaled[content_indices]\n",
        "        model_scores = model.predict(content_features).flatten()\n",
        "        model_std = np.std(model_scores)\n",
        "        model_scores_normalized = [score / model_std for score in model_scores]\n",
        "\n",
        "        for content_id, model_score in zip(combined_recommendations.keys(), model_scores_normalized):\n",
        "            combined_recommendations[content_id] += weight_model * model_score\n",
        "\n",
        "    filtered_recommendations = {content_id: score for content_id, score in combined_recommendations.items()\n",
        "                                if content.loc[content['Title'] == content_id, 'Rating Value'].values[0] >= 3.5}\n",
        "\n",
        "    final_recommendations = sorted(filtered_recommendations.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [content_id for content_id, _ in final_recommendations[:top_n]]\n",
        "\n",
        "# 예시 사용자 입력 데이터\n",
        "new_user_mbti = \"INTP\"\n",
        "new_user_preferred_contents = ['해리 포터와 혼혈왕자', '신과함께-인과 연', '미녀는 괴로워', '마이데몬']\n",
        "\n",
        "# 사용자 임베딩\n",
        "user_embedding = mbti_embeddings_dict[new_user_mbti]\n",
        "\n",
        "# 최적화된 결합 추천 시스템 실행\n",
        "start_time = time.time()\n",
        "filtered_recommendations = recommend_contents_combined_with_popularity_filter(user_embedding, new_user_preferred_contents, contents_embeddings_dict, best_gbm, content, cbf_model_input_scaled, top_n=20)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"대중성 필터링을 추가한 추천 콘텐츠 목록:\", filtered_recommendations)\n",
        "print(f\"추천 시스템 실행 시간: {end_time - start_time:.2f} 초\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VDxvDrI_oHk"
      },
      "source": [
        "**재추천을 포함한 최종 추천 시스템**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5JrjsW-eZ72",
        "outputId": "bffbd685-5ce5-4e14-c8d0-789db1ebf37e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "첫 번째 추천 콘텐츠 목록: ['네 멋대로 해라', '브루스 올마이티', '늑대소년', '주술회전', '지금 만나러 갑니다', '럭키', '데자뷰', '서약', '러브 액츄얼리', '꿈의 제인', '매기스 플랜', '레터스 투 줄리엣', '우리들', '블루 발렌타인', '색, 계', '왕의 남자', '캐리비안의 해적: 망자의 함', '본 슈프리머시', '라이온 킹', '나우 이즈 굿']\n",
            "추천 시스템 실행 시간: 1.20 초\n",
            "첫 번째 추천 목록에서 선호하는 콘텐츠를 쉼표로 구분하여 입력하세요: 지금 만나러 갑니다, 러브 액츄얼리, 나우 이즈 굿, 꿈의 제인, 블루 발렌타인\n",
            "재추천 콘텐츠 목록: ['말아톤', '늑대소녀와 흑왕자', '내 아내의 모든 것', '그녀', '바닐라 스카이', '아저씨', '지금, 만나러 갑니다', '메종 드 히미코', '시월애', '도망자', '클래식', '너의 이름은.', '시애틀의 잠 못 이루는 밤', '인생은 아름다워', '우리도 사랑일까', '죽거나 혹은 나쁘거나', '와일드', '히로인 실격', '존 윅', '검은 사제들']\n",
            "재추천 시스템 실행 시간: 1.23 초\n"
          ]
        }
      ],
      "source": [
        "# 최종 추천 시스템 (재추천 포함)\n",
        "def recommend_contents_combined_excluding_with_popularity_filter(user_embedding, preferred_contents, previous_recommendations, initial_preferred_contents, content_embeddings_dict, model, content, cbf_model_input_scaled, top_n=20):\n",
        "    # 가중치 설정\n",
        "    weight_mbti = 0.4\n",
        "    weight_similar = 0.5\n",
        "    weight_model = 0.1\n",
        "\n",
        "    # 초기 선호 콘텐츠와 현재 선호 콘텐츠 합치기\n",
        "    all_preferred_contents = list(set(preferred_contents + initial_preferred_contents))\n",
        "\n",
        "    # 대중성 점수 필터링 (Normalized Popularity Score 기준 상위 90%)\n",
        "    popularity_threshold = content['Normalized Popularity Score'].quantile(0.9)\n",
        "    popular_content = content[content['Normalized Popularity Score'] >= popularity_threshold]\n",
        "\n",
        "    # 사용자가 선호하는 콘텐츠는 필터링에서 제외하고 포함시킴\n",
        "    popular_content_embeddings_dict = {title: emb for title, emb in content_embeddings_dict.items() if title in popular_content['Title'].values or title in all_preferred_contents}\n",
        "\n",
        "    # MBTI와 유사 콘텐츠 추천 상위 100개 가져오기\n",
        "    mbti_recommendations = recommend_contents_by_mbti(user_embedding, popular_content_embeddings_dict, top_n=100)\n",
        "\n",
        "    # 유사도 계산\n",
        "    similar_contents = {}\n",
        "    for preferred_content in all_preferred_contents:\n",
        "        if preferred_content in content_embeddings_dict:\n",
        "            preferred_embedding = content_embeddings_dict[preferred_content]\n",
        "            for other_content, other_embedding in popular_content_embeddings_dict.items():\n",
        "                if other_content not in all_preferred_contents:\n",
        "                    similarity = np.dot(preferred_embedding, other_embedding) / (np.linalg.norm(preferred_embedding) * np.linalg.norm(other_embedding))\n",
        "                    if other_content not in similar_contents:\n",
        "                        similar_contents[other_content] = 0\n",
        "                    similar_contents[other_content] += similarity\n",
        "\n",
        "    similar_content_recommendations = sorted(similar_contents.items(), key=lambda x: x[1], reverse=True)[:100]\n",
        "\n",
        "    # 추천 콘텐츠를 저장할 딕셔너리 초기화\n",
        "    combined_recommendations = {}\n",
        "\n",
        "    # 모든 추천 콘텐츠 ID 집합 생성 & 중복 제거\n",
        "    all_recommendations = set([content_id for content_id, _ in mbti_recommendations] +\n",
        "                              [content_id for content_id, _ in similar_content_recommendations])\n",
        "\n",
        "    # 이전에 추천된 콘텐츠와 사용자의 초기 선호 콘텐츠 필터링\n",
        "    filtered_content_ids = {content_id for content_id in all_recommendations\n",
        "                            if content_id not in previous_recommendations and\n",
        "                            content_id not in all_preferred_contents}\n",
        "\n",
        "    # 필터링된 콘텐츠의 추천 점수 초기화\n",
        "    for content_id in filtered_content_ids:\n",
        "        combined_recommendations[content_id] = 0\n",
        "\n",
        "    # MBTI 유사도 점수 보정\n",
        "    mbti_scores = [score for content_id, score in mbti_recommendations if content_id in filtered_content_ids]\n",
        "    mbti_std = np.std(mbti_scores)\n",
        "    mbti_scores_normalized = [score / mbti_std for score in mbti_scores]\n",
        "\n",
        "    # 필터링된 콘텐츠의 MBTI 점수 가중치 적용\n",
        "    for (content_id, _), score in zip(mbti_recommendations, mbti_scores_normalized):\n",
        "        if content_id in filtered_content_ids:\n",
        "            combined_recommendations[content_id] += weight_mbti * score\n",
        "\n",
        "    # 선호 콘텐츠 유사도 점수 보정\n",
        "    similar_scores = [score for content_id, score in similar_content_recommendations if content_id in filtered_content_ids]\n",
        "    similar_std = np.std(similar_scores)\n",
        "    similar_scores_normalized = [score / similar_std for score in similar_scores]\n",
        "\n",
        "    # 필터링된 콘텐츠의 유사 콘텐츠 점수 가중치 적용\n",
        "    for (content_id, _), score in zip(similar_content_recommendations, similar_scores_normalized):\n",
        "        if content_id in filtered_content_ids:\n",
        "            combined_recommendations[content_id] += weight_similar * score\n",
        "\n",
        "    # 모델 점수 보정\n",
        "    # 필터링된 콘텐츠 인덱스\n",
        "    content_indices = [content.index[content['Title'] == content_id][0] for content_id in filtered_content_ids]\n",
        "    if content_indices:\n",
        "        # 모델에 입력할 스케일링된 피쳐\n",
        "        content_features = cbf_model_input_scaled[content_indices]\n",
        "        model_scores = model.predict(content_features).flatten()\n",
        "        model_std = np.std(model_scores)\n",
        "        model_scores_normalized = [score / model_std for score in model_scores]\n",
        "\n",
        "        # 필터링된 콘텐츠의 모델 점수 가중치 적용\n",
        "        for content_id, model_score in zip(filtered_content_ids, model_scores_normalized):\n",
        "            combined_recommendations[content_id] += weight_model * model_score\n",
        "\n",
        "    # 평점이 3.5 이상인 콘텐츠만 필터링\n",
        "    filtered_recommendations = {content_id: score for content_id, score in combined_recommendations.items()\n",
        "                                if content.loc[content['Title'] == content_id, 'Rating Value'].values[0] >= 3.5}\n",
        "\n",
        "    # 점수 내림차순으로 정렬 -> 최종 추천 콘텐츠 ID 리턴\n",
        "    final_recommendations = sorted(filtered_recommendations.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [content_id for content_id, _ in final_recommendations[:top_n]]\n",
        "\n",
        "# 예시 사용자 입력 데이터\n",
        "new_user_mbti = \"INFP\"\n",
        "new_user_preferred_contents = ['라라랜드', '500일의 썸머', '인터스텔라', '이터널 선샤인', '헤어질 결심', '시카리오: 암살자들의 도시', '남한산성']\n",
        "\n",
        "# 첫 번째 추천 시스템 실행\n",
        "start_time = time.time()\n",
        "user_embedding = mbti_embeddings_dict[new_user_mbti]\n",
        "initial_recommendations = recommend_contents_combined_with_popularity_filter(user_embedding, new_user_preferred_contents, contents_embeddings_dict, best_gbm, content, cbf_model_input_scaled, top_n=20)\n",
        "end_time = time.time()\n",
        "print(\"첫 번째 추천 콘텐츠 목록:\", initial_recommendations)\n",
        "print(f\"추천 시스템 실행 시간: {end_time - start_time:.2f} 초\")\n",
        "\n",
        "# 첫 번째 추천으로부터 재추천\n",
        "liked_contents_input = input(\"첫 번째 추천 목록에서 선호하는 콘텐츠를 쉼표로 구분하여 입력하세요: \")\n",
        "liked_contents = [content.strip() for content in liked_contents_input.split(',')]\n",
        "\n",
        "previous_recommendations = initial_recommendations\n",
        "new_preferred_contents = liked_contents  # 사용자 입력을 기반으로 새로운 선호 콘텐츠 목록 생성\n",
        "\n",
        "# 재추천 시스템 실행 시간 측정\n",
        "start_time = time.time()\n",
        "second_recommendations = recommend_contents_combined_excluding_with_popularity_filter(user_embedding, new_preferred_contents, previous_recommendations, new_user_preferred_contents, contents_embeddings_dict, best_gbm, content, cbf_model_input_scaled, top_n=20)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"재추천 콘텐츠 목록:\", second_recommendations)\n",
        "print(f\"재추천 시스템 실행 시간: {end_time - start_time:.2f} 초\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9o9qwwP4Oow"
      },
      "source": [
        "# 추천 시스템 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAdhzW-d4W-T"
      },
      "source": [
        "- NDCG(Normalized Discounted Cumulative Gain)\n",
        "  - 추천 시스템의 성능을 평가하는데 널리 사용되는 지표\n",
        "  - 추천 항목의 순위와 관련성 모두 고려하여 측정\n",
        "  - NDCG@k는 상위 k개의 추천 항목에 대해 NDCG를 계산한 값\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdxvi4MhQ-mD",
        "outputId": "eb038e31-08f4-44b1-b560-c8e1085027ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG@20 for user1: 0.6259276783324935\n",
            "NDCG@20 for user2: 0.6803051600751515\n",
            "NDCG@20 for user3: 0.8306441614162442\n",
            "NDCG@20 for user4: 0.5897274270918277\n",
            "NDCG@20 for user5: 0.5714285040141098\n",
            "NDCG@20 for user6: 0.49988395779549066\n",
            "NDCG@20 for user7: 0.48815540478337316\n",
            "NDCG@20 for user8: 0.811514705905686\n",
            "NDCG@20 for user9: 0.5510096187094674\n",
            "NDCG@20 for user10: 0.5778310654895317\n",
            "NDCG@20 for user11: 0.6002760303981841\n",
            "Average NDCG@20: 0.6206\n"
          ]
        }
      ],
      "source": [
        "# DCG 계산 함수\n",
        "def dcg_at_k(r, k):\n",
        "    r = np.asfarray(r)[:k]\n",
        "    if r.size:\n",
        "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
        "    return 0.\n",
        "\n",
        "# NDCG 계산 함수\n",
        "def ndcg_at_k(r, k):\n",
        "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
        "    if not dcg_max:\n",
        "        return 0.\n",
        "    return dcg_at_k(r, k) / dcg_max\n",
        "\n",
        "# 사용자별 실제 선호 콘텐츠와 추천 결과\n",
        "actual_preferences = {\n",
        "    'user1': ['해리 포터와 죽음의 성물 2', '맨 인 블랙', '콘스탄틴', '헝거게임: 캣칭 파이어', '서유기: 선리기연','어바웃 타임', '스파이더맨 2', '겨울왕국'],\n",
        "    'user2': ['검사외전', '부당거래', '월드워Z', '그 시절, 우리가 좋아했던 소녀', '늑대소년'],\n",
        "    'user3': ['월드워Z', '매트릭스 3: 레볼루션', '폼포코 너구리 대작전', '엣지 오브 투모로우', '가디언즈 오브 갤럭시 Vol. 2', '미이라', '토르: 다크 월드'],\n",
        "    'user4': ['메멘토', '이미테이션 게임','에놀라 홈즈', '겨울왕국 2'],\n",
        "    'user5': ['주술회전', '너의 이름은.'],\n",
        "    'user6': ['쩐의 전쟁', '미녀와 야수', '메이즈 러너', '너의 이름은.', '소울', '타이타닉'],\n",
        "    'user7': ['너의 이름은.', '하이큐!!', '나츠메 우인장', '심야식당 2'],\n",
        "    'user8': ['맨 인 블랙', '아토믹 블론드', '아이언맨 3', '인셉션', '매트릭스 3: 레볼루션', '스파이더맨: 파 프롬 홈', '맨 인 블랙 3'],\n",
        "    'user9': ['해리 포터와 비밀의 방', '러브레터', '해리 포터와 아즈카반의 죄수', '어메이징 스파이더맨', '런닝맨', '해리 포터와 불사조 기사단', '스파이더맨 2'],\n",
        "    'user10': ['500일의 썸머', '짱구는 못말려 극장판: 액션가면 VS 그래그래 마왕', '드래곤 길들이기 2', '미스 페레그린과 이상한 아이들의 집', '소울', '어메이징 스파이더맨 2', '가디언즈 오브 갤럭시 Vol. 2', '콘스탄틴'],\n",
        "    'user11': ['지금 만나러 갑니다', '러브 액츄얼리', '꿈의 제인', '매기스 플랜', '우리들', '블루 발렌타인', '색, 계', '캐리비안의 해적: 망자의 함', '본 슈프리머시', '나우 이즈 굿']\n",
        "}\n",
        "\n",
        "recommendations = {\n",
        "    'user1': ['달의 연인 - 보보경심 려', '본 얼티메이텀', '소스 코드', '해리 포터와 죽음의 성물 2', '반지의 제왕: 반지 원정대', '맨 인 블랙', '콘스탄틴', '헝거게임: 캣칭 파이어', '서유기: 선리기연', '드래곤 길들이기 3', '블랙 위도우', '반지의 제왕: 두 개의 탑', '어바웃 타임', '반지의 제왕: 왕의 귀환', '말레피센트', '브루스 올마이티', '스파이더맨 2', '겨울왕국', '원펀맨', '하이힐'],\n",
        "    'user2': ['검사외전', '내가 살인범이다', '네 멋대로 해라', '주술회전', '레터스 투 줄리엣', '13층', '헤어질 결심', '브루스 올마이티', '콘택트', '부당거래', '시애틀의 잠 못 이루는 밤', '판의 미로: 오필리아와 세개의 열쇠', '무간도 2: 혼돈의 시대', '짱구는 못말려 극장판: 태풍을 부르는 장엄한 전설의 전투', '월드워Z', '사랑의 블랙홀', '죄 많은 소녀', '그 시절, 우리가 좋아했던 소녀', '늑대소년', '옥탑방 고양이'],\n",
        "    'user3': ['월드워Z', '판의 미로: 오필리아와 세개의 열쇠', '매트릭스 3: 레볼루션', '주술회전', '라이온 킹', '폼포코 너구리 대작전', '미녀와 야수', '엣지 오브 투모로우', '가디언즈 오브 갤럭시 Vol. 2', '인어공주', '미이라', '토르: 다크 월드', '겨울왕국 2', '서유기: 선리기연', '날씨의 아이', '브이 포 벤데타', '반지의 제왕: 왕의 귀환', '늑대소년', '스타워즈 에피소드 1: 보이지 않는 위험', '짱구는 못말려 극장판: 암흑 마왕 대추적'],\n",
        "    'user4': ['아토믹 블론드', '바스터즈: 거친 녀석들', '메멘토', '이미테이션 게임', '미션 임파서블', '캡틴 아메리카: 윈터 솔져', '블러드 다이아몬드', '페이스 오프', '에놀라 홈즈', '천사와 악마', '겨울왕국 2', '미션 임파서블: 폴아웃', '스파이더맨 2', '앤트맨', '트루 라이즈', '스파이 브릿지', '혹성탈출: 진화의 시작', '프레스티지', '스타워즈 에피소드 2: 클론의 습격', '앤트맨과 와스프'],\n",
        "    'user5': ['늑대소년', '주술회전', '데자뷰', '용의자 X의 헌신', '브루스 올마이티', '죄 많은 소녀', '몬스터 주식회사', '플레이스 비욘드 더 파인즈', '너의 이름은.', '서치', '메이즈 러너', '괴물의 아이', '기생수', '나츠메 우인장', '네 멋대로 해라', '서유기: 선리기연', '이 멋진 세계에 축복을!', '토르: 다크 월드', '에이리언: 커버넌트', '콰이어트 플레이스 2'],\n",
        "    'user6': ['파이트 클럽', '브루스 올마이티', '늑대소년', '주술회전', '팀 버튼의 크리스마스 악몽', '네 멋대로 해라', '쩐의 전쟁', '블루 발렌타인', '인크레더블', '미녀와 야수', '메이즈 러너', '트랜스포머', '보글보글 스폰지밥', '너의 이름은.', '소울', '타이타닉', '콘스탄틴', '기생수', '시스터 액트', '다크 나이트'],\n",
        "    'user7': ['늑대소년', '주술회전', '짱구는 못말려 극장판: 액션가면 VS 그래그래 마왕', '너의 이름은.', '날씨의 아이', '갓파쿠와 여름방학을', '언어의 정원', '별을 쫓는 아이: 아가르타의 전설', '하이큐!!', '메종 드 히미코', '나츠메 우인장', '짱구는 못말려 극장판: 부리부리 왕국의 보물', '쉘 위 댄스', '세상의 중심에서 사랑을 외치다', '혐오스런 마츠코의 일생', '모모와 다락방의 수상한 요괴들', '심야식당 2', '시', '초속 5센티미터', '말아톤'],\n",
        "    'user8': ['맨 인 블랙', '몬스터 주식회사', '아토믹 블론드', '본 얼티메이텀', '브이 포 벤데타', '파이트 클럽', '아이언맨 3', '인셉션', '매트릭스 3: 레볼루션', '메이즈 러너', '트랜스포머', '터미네이터 2: 심판의 날', '스파이더맨: 파 프롬 홈', '미스 페레그린과 이상한 아이들의 집', '스파이더맨: 뉴 유니버스', '미션 임파서블: 데드 레코닝 PART ONE', '맨 인 블랙 3', '스파이더맨 3', '본 슈프리머시', '셜록 홈즈: 그림자 게임'],\n",
        "    'user9': ['말아톤', '늑대소년', '해리 포터와 비밀의 방', '탐정: 더 비기닝', '죄 많은 소녀', '시', '새벽의 연화', '러브레터', '호리미야', '아가씨', '릴리 슈슈의 모든 것', '미녀와 야수', '날씨의 아이', '히로인 실격', '해리 포터와 아즈카반의 죄수', '어메이징 스파이더맨', '이지 A', '런닝맨', '해리 포터와 불사조 기사단', '스파이더맨 2'],\n",
        "    'user10': ['늑대소년', '주술회전', '팀 버튼의 크리스마스 악몽', '사랑에 대한 모든 것', '500일의 썸머', '몬스터 주식회사', '괴물의 아이', '짱구는 못말려 극장판: 액션가면 VS 그래그래 마왕', '드래곤 길들이기 2', '미스 페레그린과 이상한 아이들의 집', '소울', '토이 스토리', '체인소 맨', '토이 스토리 4', '어메이징 스파이더맨 2', '토르: 다크 월드', '피터 팬', '가디언즈 오브 갤럭시 Vol. 2', '서유기: 선리기연', '콘스탄틴'],\n",
        "    'user11': ['네 멋대로 해라', '브루스 올마이티', '늑대소년', '주술회전', '지금 만나러 갑니다', '럭키', '데자뷰', '서약', '러브 액츄얼리', '꿈의 제인', '매기스 플랜', '레터스 투 줄리엣', '우리들', '블루 발렌타인', '색, 계', '왕의 남자', '캐리비안의 해적: 망자의 함', '본 슈프리머시', '라이온 킹', '나우 이즈 굿']\n",
        "}\n",
        "\n",
        "# 사용자별 NDCG 계산\n",
        "k = 20\n",
        "ndcg_scores = []\n",
        "\n",
        "for user, actual in actual_preferences.items():\n",
        "    recommended = recommendations[user]\n",
        "    relevance = [1 if item in actual else 0 for item in recommended]\n",
        "    ndcg = ndcg_at_k(relevance, k)\n",
        "    ndcg_scores.append(ndcg)\n",
        "    print(f\"NDCG@{k} for {user}: {ndcg}\")\n",
        "\n",
        "# 평균 NDCG\n",
        "average_ndcg = np.mean(ndcg_scores)\n",
        "print(f\"Average NDCG@{k}: {average_ndcg:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EhG9HSV5czu"
      },
      "source": [
        "- 평균 NDCG@20 점수가 0.6206이라는 것은 추천 시스템이 평균적으로 상위 20개의 추천 항목에서 62.06%의 이상적인 관련성을 가진다는 것을 의미합니다.\n",
        "- 이는 추천 시스템이 꽤 효과적으로 작동하고 있음을 시사합니다.\n",
        "- 하지만 아직 낮은 점수를 기록한 사용자들에 대한 개선이 가능하고 필요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhNn-BysvAcf"
      },
      "source": [
        "# 7. MVTI 추천 시스템의 특징 및 장점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXwgXcAjvFBT"
      },
      "source": [
        "**특징 및 장점**\n",
        "- **다중 기준 결합(하이브리드 추천)**: MBTI 임베딩 유사도, 선호 콘텐츠 임베딩 유사도, 모델 예측 점수, 대중성 점수를 결합하여 다양한 측면에서 최적의 콘텐츠를 추천합니다.\n",
        "- **개인화된 추천**: 사용자의 MBTI 특성과 사용자 선호 콘텐츠를 반영하여 개인화된 추천을 제공합니다.\n",
        "- **인기 콘텐츠 추천**: 정규화된 대중성 점수를 사용하여 상위 90% 대중성을 가진 콘텐츠를 우선적으로 추천하여 사용자 만족도를 높입니다.\n",
        "- **동적 추천 시스템(재추천)**: 사용자가 이전 추천 목록에서 선택한 콘텐츠를 반영하고 이전에 추천 받은 콘텐츠는 제외하여 새로운 추천 리스트를 제공합니다.\n",
        "- **확장 가능성**: 유저 행동 데이터 및 SNS 데이터 등의 다양한 데이터 소스를 통합하여 더 정교한 추천 시스템으로 확장할 수 있습니다.\n",
        "\n",
        "**평가 결과: Average NDCG@20: 0.6206**\n",
        "- **평가 지표**: NDCG는 추천된 콘텐츠의 순서와 실제 선호도를 비교하여 점수를 매기는 지표로, 높은 점수일수록 더 정확한 추천을 의미합니다.\n",
        "- **평균 NDCG@20 점수**: 0.6206은 추천 시스템이 사용자 선호도를 잘 반영하고 있음을 나타냅니다. 이는 추천된 상위 20개의 콘텐츠 중 다수가 실제로 사용자가 선호하는 콘텐츠임을 의미합니다.\n",
        "- **개선 여지**: 비록 0.6206이라는 점수가 나쁘지 않지만, 더 높은 점수를 목표로 추천 알고리즘의 튜닝 및 추가 기능 개선이 필요할 수 있습니다.\n",
        "\n",
        "**결론**\n",
        ": MBTI 추천 시스템은 다양한 추천 기법을 결합하여 사용자 맞춤형 콘텐츠를 정밀하고 다양하게 추천합니다. 이를 위해 MBTI 임베딩 유사도, 선호 콘텐츠 임베딩 유사도, 모델 예측 점수를 결합하여 최적의 콘텐츠를 제공합니다. 또한, 정규화된 대중성 점수를 통해 상위 90%의 대중성을 가진 콘텐츠를 우선적으로 추천하여 사용자 만족도를 높입니다.\n",
        "\n",
        "이 시스템은 사용자의 MBTI 특성과 선호 콘텐츠를 반영한 개인화된 추천과 사용자의 이전 선택을 반영한 동적 추천을 제공합니다. 이러한 기능은 사용자 경험을 더욱 향상시키며, 유저 행동 데이터와 SNS 데이터를 통합하여 더 정교한 추천 시스템으로 확장할 수 있는 가능성도 갖추고 있습니다.\n",
        "\n",
        "평가 결과, 평균 NDCG@20 점수는 0.6206으로, 이는 시스템이 사용자 선호도를 잘 반영하고 있음을 나타냅니다. 향후 더 높은 점수를 목표로 알고리즘의 튜닝 및 추가 기능 개선이 필요할 수 있습니다.\n",
        "\n",
        "종합적으로, MBTI 추천 시스템은 고성능 임베딩 모델과 다양한 데이터 소스를 활용하여 사용자에게 가장 적합한 콘텐츠를 제공하는 효과적인 추천 시스템입니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
